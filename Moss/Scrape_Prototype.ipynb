{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f783e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import parse_qs, urlencode, urlparse, urlunparse, urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3749c2d",
   "metadata": {},
   "source": [
    "## Initiate basic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c545f229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search URLs:\n",
      "JobThai:\n",
      "  ['Data Scientist', 'https://www.jobthai.com/th/jobs?keyword=Data%20Scientist&page=1&orderBy=RELEVANCE_SEARCH']\n",
      "  ['Data Analyst', 'https://www.jobthai.com/th/jobs?keyword=Data%20Analyst&page=1&orderBy=RELEVANCE_SEARCH']\n",
      "  ['Data Engineer', 'https://www.jobthai.com/th/jobs?keyword=Data%20Engineer&page=1&orderBy=RELEVANCE_SEARCH']\n",
      "JobsDB:\n",
      "  ['Data Scientist', 'https://th.jobsdb.com/th/Data-Scientist-jobs']\n",
      "  ['Data Analyst', 'https://th.jobsdb.com/th/Data-Analyst-jobs']\n",
      "  ['Data Engineer', 'https://th.jobsdb.com/th/Data-Engineer-jobs']\n",
      "JOBBKK:\n",
      "  ['Data Scientist', 'https://jobbkk.com/jobs/lists/1/หางาน,Data%20Scientist,ทุกจังหวัด,ทั้งหมด.html?keyword_type=3&sort=4']\n",
      "  ['Data Analyst', 'https://jobbkk.com/jobs/lists/1/หางาน,Data%20Analyst,ทุกจังหวัด,ทั้งหมด.html?keyword_type=3&sort=4']\n",
      "  ['Data Engineer', 'https://jobbkk.com/jobs/lists/1/หางาน,Data%20Engineer,ทุกจังหวัด,ทั้งหมด.html?keyword_type=3&sort=4']\n"
     ]
    }
   ],
   "source": [
    "JOBS_LIST = ['Data Scientist', 'Data Analyst', 'Data Engineer']\n",
    "# JOBS_LIST = ['Data Scientist'] #FOR DEBUGGING\n",
    "SKILLS = {\n",
    "    # ---------------- Core Programming ----------------\n",
    "    \"python\": [\"python\"],\n",
    "    \"r\": [\" r \", \" r,\", \" r\\n\", \" r/\"],\n",
    "    \"java\": [\"java\"],\n",
    "    \"scala\": [\"scala\"],\n",
    "    \"c++\": [\"c++\"],\n",
    "\n",
    "    # ---------------- SQL & Databases ----------------\n",
    "    \"sql & database\": [\" sql \", \"mysql\", \"postgres\", \"postgresql\", \"oracle\", \"sql server\", \"mssql\", \"sqlite\"],\n",
    "    \"mongodb\": [\"mongodb\", \"mongo\"],\n",
    "    \"elasticsearch\": [\"elasticsearch\", \"elastic search\"],\n",
    "\n",
    "    # ---------------- Data Libraries ----------------\n",
    "    \"pandas\": [\"pandas\"],\n",
    "    \"numpy\": [\"numpy\"],\n",
    "    \"scipy\": [\"scipy\"],\n",
    "    \"sklearn\": [\"scikit-learn\", \"sklearn\"],\n",
    "\n",
    "    # ---------------- Machine Learning ----------------\n",
    "    \"machine_learning\": [\n",
    "        \"machine learning\", \"supervised\", \"unsupervised\",\n",
    "        \"random forest\", \"xgboost\", \"lightgbm\", \"catboost\"\n",
    "    ],\n",
    "\n",
    "    # ---------------- Deep Learning ----------------\n",
    "    \"deep_learning\": [\n",
    "        \"deep learning\", \"neural network\", \"cnn\", \"rnn\", \"lstm\", \"transformer\"\n",
    "    ],\n",
    "\n",
    "    # ---------------- GenAI / LLM ----------------\n",
    "    \"llm\": [\"llm\", \"large language model\"],\n",
    "    \"rag\": [\"rag\", \"retrieval augmented generation\"],\n",
    "    \"langchain\": [\"langchain\"],\n",
    "    \"openai\": [\"openai\"],\n",
    "    \"huggingface\": [\"huggingface\"],\n",
    "    \"prompt_engineering\": [\"prompt engineering\"],\n",
    "    \"vector_db\": [\"vector database\", \"pinecone\", \"faiss\", \"weaviate\", \"milvus\"],\n",
    "\n",
    "    # ---------------- Visualization / BI ----------------\n",
    "    \"excel\": [\"excel\", \"vlookup\", \"pivot table\", \"power query\"],\n",
    "    \"powerbi\": [\"power bi\", \"powerbi\", \"dax\"],\n",
    "    \"tableau\": [\"tableau\"],\n",
    "    \"matplotlib\": [\"matplotlib\"],\n",
    "    \"seaborn\": [\"seaborn\"],\n",
    "    \"plotly\": [\"plotly\"],\n",
    "\n",
    "    # ---------------- Big Data ----------------\n",
    "    \"spark\": [\"spark\", \"pyspark\"],\n",
    "    \"hadoop\": [\"hadoop\"],\n",
    "    \"kafka\": [\"kafka\"],\n",
    "\n",
    "    # ---------------- Cloud ----------------\n",
    "    \"aws\": [\"aws\", \"amazon web services\", \"s3\", \"redshift\", \"athena\", \"glue\", \"lambda\"],\n",
    "    \"gcp\": [\"gcp\", \"google cloud\", \"bigquery\", \"cloud storage\"],\n",
    "    \"azure\": [\"azure\", \"synapse\", \"databricks\"],\n",
    "\n",
    "    # ---------------- Data Engineering ----------------\n",
    "    \"etl\": [\"etl\", \"elt\", \"data pipeline\"],\n",
    "    \"airflow\": [\"airflow\"],\n",
    "\n",
    "    # ---------------- MLOps / Deployment ----------------\n",
    "    \"docker\": [\"docker\"],\n",
    "    \"kubernetes\": [\"kubernetes\", \"k8s\"],\n",
    "    \"mlflow\": [\"mlflow\"],\n",
    "    \"fastapi\": [\"fastapi\"],\n",
    "    \"flask\": [\"flask\"],\n",
    "    \"streamlit\": [\"streamlit\"],\n",
    "\n",
    "    # ---------------- Statistics ----------------\n",
    "    \"statistics\": [\n",
    "        \"statistics\", \"statistical\", \"hypothesis testing\",\n",
    "        \"regression\", \"anova\", \"probability\"\n",
    "    ],\n",
    "\n",
    "    # ---------------- Version Control ----------------\n",
    "    \"git\": [\"git\", \"github\", \"gitlab\"],\n",
    "\n",
    "    # ---------------- APIs ----------------\n",
    "    \"api\": [\"api\", \"rest api\"],\n",
    "\n",
    "    # ---------------- Linux ----------------\n",
    "    \"linux\": [\"linux\", \"unix\"],    \n",
    "}\n",
    "\n",
    "SEARCH_URLS = {\n",
    "    \"JobThai\": [[job_title,f\"https://www.jobthai.com/th/jobs?keyword={job_title}&page=1&orderBy=RELEVANCE_SEARCH\".replace(\" \", \"%20\")] for job_title in JOBS_LIST],\n",
    "    \"JobsDB\": [[job_title,f\"https://th.jobsdb.com/th/{job_title}-jobs\".replace(\" \", \"-\")] for job_title in JOBS_LIST],\n",
    "    \"JOBBKK\": [[job_title,f\"https://jobbkk.com/jobs/lists/1/หางาน,{job_title},ทุกจังหวัด,ทั้งหมด.html?keyword_type=3&sort=4\".replace(\" \", \"%20\")] for job_title in JOBS_LIST],\n",
    "}\n",
    "\n",
    "KEY_VARIANTS = {\n",
    "    \"data\": [\"data\"],\n",
    "    \"scientist\": [\"scientist\", \"science\", \"scien\", \"scient\"],\n",
    "    \"engineer\": [\"engineer\", \"engineering\", \"eng\"],\n",
    "    \"analyst\": [\"analyst\", \"analytics\", \"analysis\"],\n",
    "    \"developer\": [\"developer\", \"development\", \"dev\"],\n",
    "}\n",
    "\n",
    "SKILL_COLUMNS = [f\"skill_{name}\" for name in SKILLS]\n",
    "\n",
    "EN_TO_THAI_PROVINCE = {\n",
    "    \"amnat charoen\": \"อำนาจเจริญ\",\n",
    "    \"ang thong\": \"อ่างทอง\",\n",
    "    \"bangkok\": \"กรุงเทพมหานคร\",\n",
    "    \"bueng kan\": \"บึงกาฬ\",\n",
    "    \"buri ram\": \"บุรีรัมย์\",\n",
    "    \"chachoengsao\": \"ฉะเชิงเทรา\",\n",
    "    \"chai nat\": \"ชัยนาท\",\n",
    "    \"chaiyaphum\": \"ชัยภูมิ\",\n",
    "    \"chanthaburi\": \"จันทบุรี\",\n",
    "    \"chiang mai\": \"เชียงใหม่\",\n",
    "    \"chiang rai\": \"เชียงราย\",\n",
    "    \"chon buri\": \"ชลบุรี\",\n",
    "    \"chonburi\": \"ชลบุรี\",\n",
    "    \"chumphon\": \"ชุมพร\",\n",
    "    \"kalasin\": \"กาฬสินธุ์\",\n",
    "    \"kamphaeng phet\": \"กำแพงเพชร\",\n",
    "    \"kanchanaburi\": \"กาญจนบุรี\",\n",
    "    \"khon kaen\": \"ขอนแก่น\",\n",
    "    \"krabi\": \"กระบี่\",\n",
    "    \"lampang\": \"ลำปาง\",\n",
    "    \"lamphun\": \"ลำพูน\",\n",
    "    \"loei\": \"เลย\",\n",
    "    \"lop buri\": \"ลพบุรี\",\n",
    "    \"lopburi\": \"ลพบุรี\",\n",
    "    \"mae hong son\": \"แม่ฮ่องสอน\",\n",
    "    \"maha sarakham\": \"มหาสารคาม\",\n",
    "    \"mukdahan\": \"มุกดาหาร\",\n",
    "    \"nakhon nayok\": \"นครนายก\",\n",
    "    \"nakhon pathom\": \"นครปฐม\",\n",
    "    \"nakhon phanom\": \"นครพนม\",\n",
    "    \"nakhon ratchasima\": \"นครราชสีมา\",\n",
    "    \"korat\": \"นครราชสีมา\",\n",
    "    \"nakhon sawan\": \"นครสวรรค์\",\n",
    "    \"nakhon si thammarat\": \"นครศรีธรรมราช\",\n",
    "    \"nan\": \"น่าน\",\n",
    "    \"narathiwat\": \"นราธิวาส\",\n",
    "    \"nong bua lamphu\": \"หนองบัวลำภู\",\n",
    "    \"nong khai\": \"หนองคาย\",\n",
    "    \"nonthaburi\": \"นนทบุรี\",\n",
    "    \"pathum thani\": \"ปทุมธานี\",\n",
    "    \"pattani\": \"ปัตตานี\",\n",
    "    \"phang nga\": \"พังงา\",\n",
    "    \"phatthalung\": \"พัทลุง\",\n",
    "    \"phayao\": \"พะเยา\",\n",
    "    \"phetchabun\": \"เพชรบูรณ์\",\n",
    "    \"phetchaburi\": \"เพชรบุรี\",\n",
    "    \"phichit\": \"พิจิตร\",\n",
    "    \"phitsanulok\": \"พิษณุโลก\",\n",
    "    \"phra nakhon si ayutthaya\": \"พระนครศรีอยุธยา\",\n",
    "    \"ayutthaya\": \"พระนครศรีอยุธยา\",\n",
    "    \"phrae\": \"แพร่\",\n",
    "    \"phuket\": \"ภูเก็ต\",\n",
    "    \"prachin buri\": \"ปราจีนบุรี\",\n",
    "    \"prachinburi\": \"ปราจีนบุรี\",\n",
    "    \"prachuap khiri khan\": \"ประจวบคีรีขันธ์\",\n",
    "    \"ranong\": \"ระนอง\",\n",
    "    \"ratchaburi\": \"ราชบุรี\",\n",
    "    \"rayong\": \"ระยอง\",\n",
    "    \"roi et\": \"ร้อยเอ็ด\",\n",
    "    \"sa kaeo\": \"สระแก้ว\",\n",
    "    \"sakaeo\": \"สระแก้ว\",\n",
    "    \"sakon nakhon\": \"สกลนคร\",\n",
    "    \"samut prakan\": \"สมุทรปราการ\",\n",
    "    \"samut sakhon\": \"สมุทรสาคร\",\n",
    "    \"samut songkhram\": \"สมุทรสงคราม\",\n",
    "    \"sara buri\": \"สระบุรี\",\n",
    "    \"saraburi\": \"สระบุรี\",\n",
    "    \"satun\": \"สตูล\",\n",
    "    \"sing buri\": \"สิงห์บุรี\",\n",
    "    \"sisaket\": \"ศรีสะเกษ\",\n",
    "    \"si sa ket\": \"ศรีสะเกษ\",\n",
    "    \"songkhla\": \"สงขลา\",\n",
    "    \"sukhothai\": \"สุโขทัย\",\n",
    "    \"suphan buri\": \"สุพรรณบุรี\",\n",
    "    \"surat thani\": \"สุราษฎร์ธานี\",\n",
    "    \"surin\": \"สุรินทร์\",\n",
    "    \"tak\": \"ตาก\",\n",
    "    \"trang\": \"ตรัง\",\n",
    "    \"trat\": \"ตราด\",\n",
    "    \"ubon ratchathani\": \"อุบลราชธานี\",\n",
    "    \"udon thani\": \"อุดรธานี\",\n",
    "    \"uthai thani\": \"อุทัยธานี\",\n",
    "    \"uttaradit\": \"อุตรดิตถ์\",\n",
    "    \"yala\": \"ยะลา\",\n",
    "    \"yasothon\": \"ยโสธร\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"th-TH,th;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "}\n",
    "\n",
    "# For Debugging Start\n",
    "print(\"Search URLs:\")\n",
    "for platform, urls in SEARCH_URLS.items():\n",
    "    print(f\"{platform}:\")\n",
    "    for url in urls:\n",
    "        print(f\"  {url}\")\n",
    "# For Debugging End     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9b526",
   "metadata": {},
   "source": [
    "## JobThai Scraper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1525dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_province_code(value) -> str:\n",
    "    text = str(value).strip()\n",
    "    if text.isdigit():\n",
    "        number = int(text)\n",
    "        if number <= 0:\n",
    "            raise ValueError(f\"Province must be positive, got: {value}\")\n",
    "        return f\"{number:02d}\"\n",
    "    raise ValueError(f\"Invalid province code: {value}\")\n",
    "\n",
    "\n",
    "def normalize_for_match(text: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \" \", text.lower()).strip()\n",
    "\n",
    "\n",
    "def normalize_for_skill_match(text: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \" \", text.lower()).strip()\n",
    "\n",
    "\n",
    "def keyword_match_groups_from_query(keyword: str) -> list[list[str]]:\n",
    "    tokens = [token for token in normalize_for_match(keyword).split() if token]\n",
    "    groups = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in KEY_VARIANTS:\n",
    "            groups.append(KEY_VARIANTS[token])\n",
    "        else:\n",
    "            groups.append([token])\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def title_matches_keyword(title: str, keyword_groups: list[list[str]]) -> bool:\n",
    "    if not keyword_groups:\n",
    "        return True\n",
    "\n",
    "    title_norm = normalize_for_match(title)\n",
    "    search_from = 0\n",
    "\n",
    "    for group in keyword_groups:\n",
    "        best_pos = None\n",
    "        best_variant = \"\"\n",
    "\n",
    "        for variant in group:\n",
    "            variant_norm = normalize_for_match(variant)\n",
    "            if not variant_norm:\n",
    "                continue\n",
    "\n",
    "            pos = title_norm.find(variant_norm, search_from)\n",
    "            if pos != -1 and (best_pos is None or pos < best_pos):\n",
    "                best_pos = pos\n",
    "                best_variant = variant_norm\n",
    "\n",
    "        if best_pos is None:\n",
    "            return False\n",
    "\n",
    "        search_from = best_pos + len(best_variant)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def variant_matches_text(variant: str, normalized_text: str) -> bool:\n",
    "    variant_norm = normalize_for_skill_match(variant)\n",
    "    if not variant_norm:\n",
    "        return False\n",
    "    pattern = rf\"(?<![a-z0-9]){re.escape(variant_norm).replace(r'\\\\ ', r'\\\\s+')}(?![a-z0-9])\"\n",
    "    return re.search(pattern, normalized_text) is not None\n",
    "\n",
    "\n",
    "def extract_skills(text: str) -> dict:\n",
    "    normalized_text = normalize_for_skill_match(text)\n",
    "    matched = []\n",
    "\n",
    "    for skill_name, variants in SKILLS.items():\n",
    "        if any(variant_matches_text(variant, normalized_text) for variant in variants):\n",
    "            matched.append(skill_name)\n",
    "\n",
    "    skill_flags = {f\"skill_{name}\": int(name in matched) for name in SKILLS}\n",
    "\n",
    "    return {\n",
    "        \"matched_skills\": \"|\".join(matched),\n",
    "        \"matched_skill_count\": len(matched),\n",
    "        **skill_flags,\n",
    "    }\n",
    "\n",
    "\n",
    "def normalize_jobthai_detail_url(job_url: str) -> str:\n",
    "    if not job_url:\n",
    "        return \"\"\n",
    "\n",
    "    parsed = urlparse(job_url)\n",
    "    path = parsed.path\n",
    "\n",
    "    path = path.replace(\"/th/company/job/\", \"/th/job/\")\n",
    "    path = path.replace(\"/company/job/\", \"/job/\")\n",
    "\n",
    "    return urlunparse((parsed.scheme, parsed.netloc, path, \"\", \"\", \"\"))\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    return \" \".join((text or \"\").split())\n",
    "\n",
    "\n",
    "def extract_salary(text: str) -> str:\n",
    "    patterns = [\n",
    "        r\"\\d[\\d,\\s]*\\s*-\\s*\\d[\\d,\\s]*\\s*บาท\",\n",
    "        r\"\\d[\\d,\\s]*\\s*บาท\",\n",
    "        r\"ตามโครงสร้างบริษัทฯ\",\n",
    "        r\"ตามประสบการณ์\",\n",
    "        r\"ตามตกลง\",\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return clean_text(match.group(0))\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_posted_date(text: str) -> str:\n",
    "    match = re.search(r\"\\b\\d{1,2}\\s+[ก-๙A-Za-z\\.]+\\s+\\d{2}\\b\", text)\n",
    "    return clean_text(match.group(0)) if match else \"\"\n",
    "\n",
    "\n",
    "def pick_text(parent, selectors: list[str]) -> str:\n",
    "    for selector in selectors:\n",
    "        element = parent.select_one(selector)\n",
    "        if element:\n",
    "            text = clean_text(element.get_text(\" \", strip=True))\n",
    "            if text:\n",
    "                return text\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def guess_location(lines: list[str], title: str, company: str, salary: str) -> str:\n",
    "    priority_keywords = [\"เขต\", \"กรุงเทพ\", \"จังหวัด\", \"อำเภอ\", \"อ.\", \"ต.\"]\n",
    "    transit_keywords = [\"BTS\", \"MRT\", \"SRT\", \"BRT\", \"Airport Rail Link\"]\n",
    "\n",
    "    for line in lines:\n",
    "        if line in {title, company, salary}:\n",
    "            continue\n",
    "        if any(keyword in line for keyword in priority_keywords):\n",
    "            return line\n",
    "\n",
    "    for line in lines:\n",
    "        if line in {title, company, salary}:\n",
    "            continue\n",
    "        if any(keyword in line for keyword in transit_keywords):\n",
    "            return line\n",
    "\n",
    "    if salary and salary in lines:\n",
    "        salary_idx = lines.index(salary)\n",
    "        for idx in range(salary_idx - 1, -1, -1):\n",
    "            candidate = lines[idx]\n",
    "            if candidate not in {title, company}:\n",
    "                return candidate\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_card_from_title(title_node, page_num: int, keyword: str) -> dict:\n",
    "    title = clean_text(title_node.get_text(\" \", strip=True))\n",
    "\n",
    "    anchor = title_node.find_parent(\"a\", href=True)\n",
    "    href = anchor.get(\"href\", \"\") if anchor else \"\"\n",
    "    job_url = href if href.startswith(\"http\") else f\"https://www.jobthai.com{href}\"\n",
    "    job_url = normalize_jobthai_detail_url(job_url)\n",
    "\n",
    "    card = anchor if anchor is not None else title_node\n",
    "\n",
    "    company = pick_text(card, [\n",
    "        'span[id^=\"job-list-company-name-\"]',\n",
    "        'h2.ohgq7e-0.enAWkF',\n",
    "    ])\n",
    "\n",
    "    location = pick_text(card, [\n",
    "        \"h3#location-text\",\n",
    "        \"h3.location-text\",\n",
    "    ])\n",
    "\n",
    "    salary = pick_text(card, [\n",
    "        \"span.salary-text\",\n",
    "        \"div.msklqa-20\",\n",
    "        \"div.msklqa-17\",\n",
    "    ])\n",
    "\n",
    "    posted_date = pick_text(card, [\n",
    "        \"span.msklqa-9\",\n",
    "    ])\n",
    "\n",
    "    raw_lines = [clean_text(x) for x in card.get_text(\"\\n\", strip=True).splitlines() if clean_text(x)]\n",
    "    raw_text = clean_text(\" \".join(raw_lines))\n",
    "\n",
    "    if not salary:\n",
    "        salary = extract_salary(raw_text)\n",
    "    if not posted_date:\n",
    "        posted_date = extract_posted_date(raw_text)\n",
    "    if not location:\n",
    "        location = guess_location(raw_lines, title=title, company=company, salary=salary)\n",
    "\n",
    "    return {\n",
    "        \"keyword\": keyword,\n",
    "        \"page\": page_num,\n",
    "        \"job_title\": title,\n",
    "        \"company\": company,\n",
    "        \"location\": location,\n",
    "        \"salary\": salary,\n",
    "        \"posted_date\": posted_date,\n",
    "        \"job_url\": job_url,\n",
    "        \"raw_text\": raw_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_detail_from_job_page(job_url: str, headers: dict) -> dict:\n",
    "    base_detail = {\n",
    "        \"province_code\": \"\",\n",
    "        \"province_name\": \"\",\n",
    "        \"job_detail_text\": \"\",\n",
    "        \"job_qualification_text\": \"\",\n",
    "        \"matched_skills\": \"\",\n",
    "        \"matched_skill_count\": 0,\n",
    "        **{column: 0 for column in SKILL_COLUMNS},\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(job_url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "    except Exception:\n",
    "        return base_detail\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    province_code = \"\"\n",
    "    province_name = \"\"\n",
    "    for anchor in soup.select('a[href*=\"province=\"]'):\n",
    "        tag = anchor.select_one('h3[id^=\"job-detail-tag-\"]')\n",
    "        if not tag:\n",
    "            continue\n",
    "\n",
    "        href = anchor.get(\"href\", \"\")\n",
    "        name = clean_text(tag.get_text(\" \", strip=True))\n",
    "        if not href or not name:\n",
    "            continue\n",
    "\n",
    "        province_value = parse_qs(urlparse(href).query).get(\"province\", [\"\"])[0]\n",
    "        if not province_value or not province_value.isdigit():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            province_code = normalize_province_code(province_value)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        province_name = name\n",
    "        break\n",
    "\n",
    "    jd_node = soup.select_one(\"span#job-detail\")\n",
    "    job_detail_text = clean_text(jd_node.get_text(\"\\n\", strip=True)) if jd_node else \"\"\n",
    "\n",
    "    qualification_node = soup.select_one(\"#job-properties-wrapper\")\n",
    "    job_qualification_text = clean_text(qualification_node.get_text(\" \", strip=True)) if qualification_node else \"\"\n",
    "\n",
    "    combined_text = \" \".join([text for text in [job_detail_text, job_qualification_text] if text])\n",
    "    skill_info = extract_skills(combined_text)\n",
    "\n",
    "    return {\n",
    "        \"province_code\": province_code,\n",
    "        \"province_name\": province_name,\n",
    "        \"job_detail_text\": job_detail_text,\n",
    "        \"job_qualification_text\": job_qualification_text,\n",
    "        **skill_info,\n",
    "    }\n",
    "\n",
    "\n",
    "def scrape_job_jobthai(\n",
    "    SEARCH_URLS: dict[str, list[str]],\n",
    "    SLEEP_SEC: float = 0.5,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    collected_frames = []\n",
    "\n",
    "    try:\n",
    "        for job in SEARCH_URLS[\"JobThai\"]:\n",
    "            keyword = job[0]\n",
    "            search_url = job[1]\n",
    "\n",
    "            keyword_groups = keyword_match_groups_from_query(keyword)\n",
    "            print(f\"Scraping JobThai for '{keyword}'\")\n",
    "\n",
    "            all_rows = []\n",
    "            seen_urls = set()\n",
    "\n",
    "            for page_no in range(1, 50):\n",
    "                page_url = search_url.replace(\"page=1\", f\"page={page_no}\")\n",
    "                print(f\"\\tFetching page {page_no}\")\n",
    "\n",
    "                response = requests.get(page_url, headers=headers, timeout=30)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                if \"nodata=true\" in response.url.lower():\n",
    "                    print(\"No data found for this keyword.\")\n",
    "                    break\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                title_cards_html = soup.select('h2[id^=\"job-card-item-\"]')\n",
    "\n",
    "                page_rows = []\n",
    "                for title_card_html in title_cards_html:\n",
    "                    row = parse_card_from_title(\n",
    "                        title_card_html,\n",
    "                        page_num=page_no,\n",
    "                        keyword=keyword,\n",
    "                    )\n",
    "\n",
    "                    if not row[\"job_url\"]:\n",
    "                        continue\n",
    "                    if not title_matches_keyword(row[\"job_title\"], keyword_groups):\n",
    "                        continue\n",
    "                    if row[\"job_url\"] in seen_urls:\n",
    "                        continue\n",
    "\n",
    "                    seen_urls.add(row[\"job_url\"])\n",
    "                    page_rows.append(row)\n",
    "\n",
    "                if not page_rows:\n",
    "                    break\n",
    "\n",
    "                all_rows.extend(page_rows)\n",
    "\n",
    "                if SLEEP_SEC > 0:\n",
    "                    time.sleep(SLEEP_SEC)\n",
    "\n",
    "            total_details = len(all_rows)\n",
    "            print(f\"[Detail] Starting detail extraction for {total_details} jobs\")\n",
    "\n",
    "            for row in all_rows:\n",
    "                detail_info = extract_detail_from_job_page(row[\"job_url\"], headers=headers)\n",
    "                row.update(detail_info)\n",
    "\n",
    "                if SLEEP_SEC > 0:\n",
    "                    time.sleep(SLEEP_SEC)\n",
    "\n",
    "            job_df = pd.DataFrame(all_rows)\n",
    "            if job_df.empty:\n",
    "                continue\n",
    "            job_df[\"domain\"] = \"JobThai\"\n",
    "            job_df[\"min_salary\"] = None\n",
    "            job_df[\"max_salary\"] = None\n",
    "\n",
    "            ordered_columns = [\n",
    "                \"domain\",\n",
    "                \"keyword\",\n",
    "                \"province_name\",\n",
    "                \"job_title\",\n",
    "                \"company\",\n",
    "                \"location\",\n",
    "                \"salary\",\n",
    "                \"min_salary\",\n",
    "                \"max_salary\",\n",
    "                \"posted_date\",\n",
    "                \"job_url\",\n",
    "                \"matched_skills\",\n",
    "                \"matched_skill_count\",\n",
    "                *SKILL_COLUMNS,\n",
    "            ]\n",
    "            for column in ordered_columns:\n",
    "                if column not in job_df.columns:\n",
    "                    job_df[column] = \"\" if column not in {\"matched_skill_count\", *SKILL_COLUMNS} else 0\n",
    "\n",
    "            job_df = job_df[ordered_columns].drop_duplicates(subset=[\"job_url\"])\n",
    "            collected_frames.append(job_df)\n",
    "            print(f\"[Done] Collected {len(job_df)} rows for '{keyword}' job search in JobThai\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred on JobThai scraping: {e}\")\n",
    "        print(\"Skipping JobThai and returning collected data so far.\")\n",
    "\n",
    "    if not collected_frames:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_df = pd.concat(collected_frames, ignore_index=True)\n",
    "    final_df = final_df.drop_duplicates(subset=[\"job_url\"])\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abcb61",
   "metadata": {},
   "source": [
    "## JobThai Scraper Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fcb800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping JobThai for 'Data Scientist'\n",
      "\tFetching page 1\n",
      "\tFetching page 2\n",
      "[Detail] Starting detail extraction for 7 jobs\n",
      "[Done] Collected 7 rows for 'Data Scientist' job search in JobThai\n",
      "Scraping JobThai for 'Data Analyst'\n",
      "\tFetching page 1\n",
      "\tFetching page 2\n",
      "\tFetching page 3\n",
      "\tFetching page 4\n",
      "\tFetching page 5\n",
      "[Detail] Starting detail extraction for 62 jobs\n",
      "[Done] Collected 62 rows for 'Data Analyst' job search in JobThai\n",
      "Scraping JobThai for 'Data Engineer'\n",
      "\tFetching page 1\n",
      "\tFetching page 2\n",
      "\tFetching page 3\n",
      "[Detail] Starting detail extraction for 27 jobs\n",
      "[Done] Collected 27 rows for 'Data Engineer' job search in JobThai\n"
     ]
    }
   ],
   "source": [
    "jobthai_scraped_df = scrape_job_jobthai(SEARCH_URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf7533",
   "metadata": {},
   "source": [
    "## Clean Data JobThai Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff456f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "THAI_MONTHS = {\n",
    "    \"ม.ค.\": 1,\n",
    "    \"ก.พ.\": 2,\n",
    "    \"มี.ค.\": 3,\n",
    "    \"เม.ย.\": 4,\n",
    "    \"พ.ค.\": 5,\n",
    "    \"มิ.ย.\": 6,\n",
    "    \"ก.ค.\": 7,\n",
    "    \"ส.ค.\": 8,\n",
    "    \"ก.ย.\": 9,\n",
    "    \"ต.ค.\": 10,\n",
    "    \"พ.ย.\": 11,\n",
    "    \"ธ.ค.\": 12,\n",
    "}\n",
    "\n",
    "def parse_thai_short_date(value: str) -> str:\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    match = re.match(r\"^(\\d{1,2})\\s+([ก-๙\\.]+)\\s+(\\d{2})$\", text)\n",
    "    if not match:\n",
    "        return \"\"\n",
    "\n",
    "    day = int(match.group(1))\n",
    "    month_text = match.group(2)\n",
    "    yy_be = int(match.group(3))  # e.g. 69 -> 2569 (B.E.)\n",
    "    month = THAI_MONTHS.get(month_text)\n",
    "    if month is None:\n",
    "        return \"\"\n",
    "\n",
    "    year_ad = (2500 + yy_be) - 543\n",
    "    try:\n",
    "        parsed = pd.Timestamp(year=year_ad, month=month, day=day)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "    return parsed.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "\n",
    "def clean_data_jobthai(job_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # province_name: remove \"จ.\" prefix\n",
    "    job_df[\"province_name\"] = (\n",
    "        job_df[\"province_name\"]\n",
    "        .fillna(\"\")\n",
    "        .astype(str)\n",
    "        .str.replace(r\"^\\s*จ\\.\\s*\", \"\", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # salary: extract min/max\n",
    "    salary_pattern = re.compile(\n",
    "        r\"^\\s*(\\d{1,3}(?:,\\d{3})*)\\s*-\\s*(\\d{1,3}(?:,\\d{3})*)(?:\\s*บาท)?\\s*$\"\n",
    "    )\n",
    "\n",
    "    salary_parts = job_df[\"salary\"].fillna(\"\").astype(str).str.extract(salary_pattern)\n",
    "    job_df[\"min_salary\"] = salary_parts[0].fillna(\"\").str.replace(\",\", \"\", regex=False)\n",
    "    job_df[\"max_salary\"] = salary_parts[1].fillna(\"\").str.replace(\",\", \"\", regex=False)\n",
    "\n",
    "    # 3) posted_date: convert Thai short date like to MM/DD/YYYY format\n",
    "    job_df[\"posted_date\"] = (\n",
    "        job_df[\"posted_date\"]\n",
    "        .fillna(\"\")\n",
    "        .astype(str)\n",
    "        .apply(parse_thai_short_date)\n",
    "    )\n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e07f91",
   "metadata": {},
   "source": [
    "## Clean & Export Scraped JobThai Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c34d9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobthai_scraped_df = clean_data_jobthai(jobthai_scraped_df)\n",
    "jobthai_scraped_df.to_csv(\"./Scraped_Each/jobthai_jobs.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e7c45",
   "metadata": {},
   "source": [
    "## JobsDB Scraper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61969486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_query_in_url(url: str, **params) -> str:\n",
    "    parsed = urlparse(url)\n",
    "    query = parse_qs(parsed.query)\n",
    "    for key, value in params.items():\n",
    "        query[key] = [str(value)]\n",
    "    new_query = urlencode(query, doseq=True)\n",
    "    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, new_query, parsed.fragment))\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (text or \"\")).strip()\n",
    "\n",
    "def normalize_for_match(text: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \" \", (text or \"\").lower()).strip()\n",
    "\n",
    "def keyword_match_groups_from_query(search_keyword: str) -> list[list[str]]:\n",
    "\n",
    "    tokens = [token for token in normalize_for_match(search_keyword).split() if token]\n",
    "    groups = []\n",
    "\n",
    "    for token in tokens:\n",
    "        groups.append(KEY_VARIANTS.get(token, [token]))\n",
    "\n",
    "    return groups\n",
    "\n",
    "def title_matches_keyword(title: str, keyword_groups: list[list[str]]) -> bool:\n",
    "    if not keyword_groups:\n",
    "        return True\n",
    "    title_norm = normalize_for_match(title)\n",
    "    return all(any(variant in title_norm for variant in group) for group in keyword_groups)\n",
    "\n",
    "def extract_salary(text: str) -> str:\n",
    "    patterns = [\n",
    "        r\"THB\\s*[\\d,]+\\s*[-–]\\s*THB\\s*[\\d,]+\",\n",
    "        r\"THB\\s*[\\d,]+\",\n",
    "        r\"[\\d,]+\\s*[-–]\\s*[\\d,]+\\s*บาท\",\n",
    "        r\"[\\d,]+\\s*บาท\",\n",
    "        r\"Negotiable|ไม่ระบุเงินเดือน|ตามตกลง|ตามประสบการณ์\",\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            return clean_text(match.group(0))\n",
    "    return \"\"\n",
    "\n",
    "def is_probable_salary(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    text_norm = text.lower()\n",
    "    salary_keywords = [\"thb\", \"บาท\", \"salary\", \"negotiable\", \"ตามตกลง\", \"ตามประสบการณ์\"]\n",
    "    if any(key in text_norm for key in salary_keywords):\n",
    "        return True\n",
    "    return bool(re.search(r\"\\d\", text_norm) and re.search(r\"[-–]\", text_norm))\n",
    "\n",
    "def guess_province_name(location_text: str) -> str:\n",
    "    location_clean = clean_text(location_text)\n",
    "    if not location_clean:\n",
    "        return \"\"\n",
    "\n",
    "    for province in EN_TO_THAI_PROVINCE.values():\n",
    "        if province in location_clean:\n",
    "            return province\n",
    "\n",
    "    location_lower = location_clean.lower()\n",
    "    for english_name, thai_name in EN_TO_THAI_PROVINCE.items():\n",
    "        if re.search(rf\"\\b{re.escape(english_name)}\\b\", location_lower):\n",
    "            return thai_name\n",
    "\n",
    "    parts = [clean_text(part) for part in re.split(r\",|\\||/\", location_clean) if clean_text(part)]\n",
    "    if not parts:\n",
    "        return \"\"\n",
    "\n",
    "    tail = parts[-1]\n",
    "    tail = re.sub(r\"^(เขต|อ\\.|อำเภอ|จ\\.|จังหวัด)\\s*\", \"\", tail).strip()\n",
    "    return tail\n",
    "\n",
    "def extract_job_detail_text(job_url: str) -> str:\n",
    "    try:\n",
    "        response = requests.get(job_url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    detail_el = soup.select_one(\"[data-automation='jobAdDetails']\")\n",
    "    if detail_el:\n",
    "        return clean_text(detail_el.get_text(\"\\n\", strip=True))\n",
    "\n",
    "    section_el = soup.select_one(\"section\")\n",
    "    if section_el:\n",
    "        return clean_text(section_el.get_text(\"\\n\", strip=True))\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def variant_matches_text(variant: str, normalized_text: str) -> bool:\n",
    "    variant_norm = normalize_for_match(variant)\n",
    "    if not variant_norm:\n",
    "        return False\n",
    "\n",
    "    pattern = re.escape(variant_norm)\n",
    "    pattern = pattern.replace(r\"\\ \", r\"\\s+\")\n",
    "    regex = rf\"(?<![a-z0-9]){pattern}(?![a-z0-9])\"\n",
    "    return re.search(regex, normalized_text) is not None\n",
    "\n",
    "\n",
    "def extract_skills(detail_text: str) -> dict:\n",
    "    text_norm = normalize_for_match(detail_text)\n",
    "    found = []\n",
    "    flags = {}\n",
    "\n",
    "    for skill_key, variants in SKILLS.items():\n",
    "        matched = any(variant_matches_text(variant, text_norm) for variant in variants)\n",
    "        flags[f\"skill_{skill_key}\"] = int(matched)\n",
    "        if matched:\n",
    "            found.append(skill_key)\n",
    "\n",
    "    flags[\"matched_skills\"] = \"|\".join(found)\n",
    "    flags[\"matched_skill_count\"] = len(found)\n",
    "    return flags\n",
    "\n",
    "def parse_card(card, page_num: int, search_keyword: str) -> dict:\n",
    "    title_el = card.select_one(\"a[data-automation='jobTitle']\")\n",
    "    company_el = card.select_one(\"a[data-automation='jobCompany'], [data-automation='jobCompany']\")\n",
    "    location_el = card.select_one(\"a[data-automation='jobLocation'], [data-automation='jobCardLocation']\")\n",
    "    date_el = card.select_one(\"[data-automation='jobListingDate']\")\n",
    "    salary_el = card.select_one(\"[data-automation='jobSalary']\")\n",
    "    overlay_link_el = card.select_one(\"a[data-automation='job-list-item-link-overlay'][href]\")\n",
    "\n",
    "    title = clean_text(title_el.get_text(\" \", strip=True) if title_el else \"\")\n",
    "    company = clean_text(company_el.get_text(\" \", strip=True) if company_el else \"\")\n",
    "    location_name = clean_text(location_el.get_text(\" \", strip=True) if location_el else \"\")\n",
    "    posted_date = clean_text(date_el.get_text(\" \", strip=True) if date_el else \"\")\n",
    "\n",
    "    salary_candidate = clean_text(salary_el.get_text(\" \", strip=True) if salary_el else \"\")\n",
    "    salary = salary_candidate if is_probable_salary(salary_candidate) else \"\"\n",
    "\n",
    "    href = \"\"\n",
    "    if overlay_link_el:\n",
    "        href = overlay_link_el.get(\"href\", \"\")\n",
    "    elif title_el and title_el.get(\"href\"):\n",
    "        href = title_el.get(\"href\", \"\")\n",
    "    job_url = urljoin(\"https://th.jobsdb.com\", href) if href else \"\"\n",
    "\n",
    "    raw_text = clean_text(card.get_text(\"\\n\", strip=True))\n",
    "    if not salary:\n",
    "        salary = extract_salary(raw_text)\n",
    "\n",
    "    province_name = guess_province_name(location_name)\n",
    "\n",
    "    return {\n",
    "        \"keyword\": search_keyword,\n",
    "        \"province_code\": \"\",\n",
    "        \"province_name\": province_name,\n",
    "        \"page\": page_num,\n",
    "        \"job_title\": title,\n",
    "        \"company\": company,\n",
    "        \"location\": location_name,\n",
    "        \"salary\": salary,\n",
    "        \"posted_date\": posted_date,\n",
    "        \"job_url\": job_url,\n",
    "        \"raw_text\": raw_text,\n",
    "    }\n",
    "\n",
    "def scrape_job_jobsdb(search_url: str = \"\", search_location: str = \"\", max_pages: int = 50, sleep_seconds: float = 0.5) -> pd.DataFrame:\n",
    "    collected_frames = []\n",
    "\n",
    "    try:\n",
    "        for job in SEARCH_URLS[\"JobsDB\"]:\n",
    "            keyword = job[0]\n",
    "            search_url = job[1]\n",
    "            keyword_groups = keyword_match_groups_from_query(keyword)\n",
    "\n",
    "            all_rows = []\n",
    "            seen_urls = set()\n",
    "\n",
    "            print(f\"[Search] Starting JobsDB crawl: max_pages={max_pages}\")\n",
    "\n",
    "            for page_num in range(1, max_pages + 1):\n",
    "                page_url = update_query_in_url(search_url, page=page_num)\n",
    "                if search_location.strip():\n",
    "                    page_url = update_query_in_url(page_url, where=search_location.strip())\n",
    "\n",
    "                print(f\"[Search] Page {page_num}/{max_pages} -> request\")\n",
    "                response = requests.get(page_url, headers=headers, timeout=30)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                cards = soup.select(\"article[data-testid='job-card'], article[data-automation='normalJob']\")\n",
    "                print(f\"[Search] Page {page_num}/{max_pages} -> found cards: {len(cards)}\")\n",
    "\n",
    "                if not cards:\n",
    "                    print(f\"[Search] Page {page_num}/{max_pages} -> no cards, stopping\")\n",
    "                    break\n",
    "\n",
    "                page_rows = []\n",
    "                for card in cards:\n",
    "                    row = parse_card(card, page_num=page_num, search_keyword=keyword)\n",
    "                    if not row[\"job_title\"] or not row[\"job_url\"]:\n",
    "                        continue\n",
    "                    if not title_matches_keyword(row[\"job_title\"], keyword_groups):\n",
    "                        continue\n",
    "                    if row[\"job_url\"] in seen_urls:\n",
    "                        continue\n",
    "\n",
    "                    seen_urls.add(row[\"job_url\"])\n",
    "                    page_rows.append(row)\n",
    "\n",
    "                if not page_rows:\n",
    "                    print(f\"[Search] Page {page_num}/{max_pages} -> no keyword matches, stopping\")\n",
    "                    break\n",
    "\n",
    "                all_rows.extend(page_rows)\n",
    "                print(f\"[Search] Page {page_num}/{max_pages} -> kept {len(page_rows)} | cumulative={len(all_rows)}\")\n",
    "\n",
    "                if sleep_seconds > 0:\n",
    "                    time.sleep(sleep_seconds)\n",
    "\n",
    "            print(f\"[Detail] Start detail scrape for {len(all_rows)} jobs\")\n",
    "\n",
    "            for idx, row in enumerate(all_rows, start=1):\n",
    "                detail_text = extract_job_detail_text(row[\"job_url\"])\n",
    "                row[\"job_detail_text\"] = detail_text\n",
    "\n",
    "                skill_result = extract_skills(detail_text)\n",
    "                row.update(skill_result)\n",
    "\n",
    "                if len(all_rows) <= 50 or idx % 10 == 0 or idx == len(all_rows):\n",
    "                    percent = (idx / len(all_rows)) * 100 if all_rows else 100\n",
    "                    print(f\"[Detail] {idx}/{len(all_rows)} ({percent:.1f}%)\")\n",
    "\n",
    "                if  sleep_seconds > 0:\n",
    "                    time.sleep(sleep_seconds)\n",
    "\n",
    "            job_df = pd.DataFrame(all_rows)\n",
    "\n",
    "            if job_df.empty:\n",
    "                continue\n",
    "\n",
    "            job_df[\"domain\"] = \"JobsDB\"\n",
    "            job_df[\"min_salary\"] = None\n",
    "            job_df[\"max_salary\"] = None\n",
    "\n",
    "            ordered_cols = [\n",
    "            \"domain\",\n",
    "            \"keyword\",\n",
    "            \"province_name\",\n",
    "            \"job_title\",\n",
    "            \"company\",\n",
    "            \"location\",\n",
    "            \"salary\",\n",
    "            \"min_salary\",\n",
    "            \"max_salary\",\n",
    "            \"posted_date\",\n",
    "            \"job_url\",\n",
    "            \"matched_skills\",\n",
    "            \"matched_skill_count\",\n",
    "            *SKILL_COLUMNS,\n",
    "            ]\n",
    "\n",
    "            for col in ordered_cols:\n",
    "                if col not in job_df.columns:\n",
    "                    job_df[col] = \"\" if not col.startswith(\"skill_\") and col != \"matched_skill_count\" else 0\n",
    "\n",
    "            job_df = job_df[ordered_cols].drop_duplicates(subset=[\"job_url\"])\n",
    "            collected_frames.append(job_df)\n",
    "            print(f\"[Done] Collected {len(job_df)} rows for '{keyword}' job search in JobsDB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred on JobsDB scraping: {e}\")\n",
    "        print(\"Skipping JobThai and returning collected data so far.\")\n",
    "\n",
    "    if not collected_frames:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    final_df = pd.concat(collected_frames, ignore_index=True)\n",
    "    final_df = final_df.drop_duplicates(subset=[\"job_url\"])\n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b82bd",
   "metadata": {},
   "source": [
    "## JobsDB Scraper Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4d30d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search] Starting JobsDB crawl: max_pages=50\n",
      "[Search] Page 1/50 -> request\n",
      "[Search] Page 1/50 -> found cards: 32\n",
      "[Search] Page 1/50 -> kept 10 | cumulative=10\n",
      "[Search] Page 2/50 -> request\n",
      "[Search] Page 2/50 -> found cards: 32\n",
      "[Search] Page 2/50 -> kept 2 | cumulative=12\n",
      "[Search] Page 3/50 -> request\n",
      "[Search] Page 3/50 -> found cards: 32\n",
      "[Search] Page 3/50 -> kept 2 | cumulative=14\n",
      "[Search] Page 4/50 -> request\n",
      "[Search] Page 4/50 -> found cards: 32\n",
      "[Search] Page 4/50 -> no keyword matches, stopping\n",
      "[Detail] Start detail scrape for 14 jobs\n",
      "[Detail] 1/14 (7.1%)\n",
      "[Detail] 2/14 (14.3%)\n",
      "[Detail] 3/14 (21.4%)\n",
      "[Detail] 4/14 (28.6%)\n",
      "[Detail] 5/14 (35.7%)\n",
      "[Detail] 6/14 (42.9%)\n",
      "[Detail] 7/14 (50.0%)\n",
      "[Detail] 8/14 (57.1%)\n",
      "[Detail] 9/14 (64.3%)\n",
      "[Detail] 10/14 (71.4%)\n",
      "[Detail] 11/14 (78.6%)\n",
      "[Detail] 12/14 (85.7%)\n",
      "[Detail] 13/14 (92.9%)\n",
      "[Detail] 14/14 (100.0%)\n",
      "[Done] Collected 14 rows for 'Data Scientist' job search in JobsDB\n",
      "[Search] Starting JobsDB crawl: max_pages=50\n",
      "[Search] Page 1/50 -> request\n",
      "[Search] Page 1/50 -> found cards: 32\n",
      "[Search] Page 1/50 -> kept 24 | cumulative=24\n",
      "[Search] Page 2/50 -> request\n",
      "[Search] Page 2/50 -> found cards: 32\n",
      "[Search] Page 2/50 -> kept 17 | cumulative=41\n",
      "[Search] Page 3/50 -> request\n",
      "[Search] Page 3/50 -> found cards: 32\n",
      "[Search] Page 3/50 -> kept 10 | cumulative=51\n",
      "[Search] Page 4/50 -> request\n",
      "[Search] Page 4/50 -> found cards: 32\n",
      "[Search] Page 4/50 -> kept 7 | cumulative=58\n",
      "[Search] Page 5/50 -> request\n",
      "[Search] Page 5/50 -> found cards: 32\n",
      "[Search] Page 5/50 -> kept 5 | cumulative=63\n",
      "[Search] Page 6/50 -> request\n",
      "[Search] Page 6/50 -> found cards: 32\n",
      "[Search] Page 6/50 -> kept 1 | cumulative=64\n",
      "[Search] Page 7/50 -> request\n",
      "[Search] Page 7/50 -> found cards: 32\n",
      "[Search] Page 7/50 -> kept 1 | cumulative=65\n",
      "[Search] Page 8/50 -> request\n",
      "[Search] Page 8/50 -> found cards: 32\n",
      "[Search] Page 8/50 -> kept 3 | cumulative=68\n",
      "[Search] Page 9/50 -> request\n",
      "[Search] Page 9/50 -> found cards: 32\n",
      "[Search] Page 9/50 -> kept 1 | cumulative=69\n",
      "[Search] Page 10/50 -> request\n",
      "[Search] Page 10/50 -> found cards: 32\n",
      "[Search] Page 10/50 -> kept 1 | cumulative=70\n",
      "[Search] Page 11/50 -> request\n",
      "[Search] Page 11/50 -> found cards: 32\n",
      "[Search] Page 11/50 -> kept 3 | cumulative=73\n",
      "[Search] Page 12/50 -> request\n",
      "[Search] Page 12/50 -> found cards: 32\n",
      "[Search] Page 12/50 -> no keyword matches, stopping\n",
      "[Detail] Start detail scrape for 73 jobs\n",
      "[Detail] 10/73 (13.7%)\n",
      "[Detail] 20/73 (27.4%)\n",
      "[Detail] 30/73 (41.1%)\n",
      "[Detail] 40/73 (54.8%)\n",
      "[Detail] 50/73 (68.5%)\n",
      "[Detail] 60/73 (82.2%)\n",
      "[Detail] 70/73 (95.9%)\n",
      "[Detail] 73/73 (100.0%)\n",
      "[Done] Collected 73 rows for 'Data Analyst' job search in JobsDB\n",
      "[Search] Starting JobsDB crawl: max_pages=50\n",
      "[Search] Page 1/50 -> request\n",
      "[Search] Page 1/50 -> found cards: 32\n",
      "[Search] Page 1/50 -> kept 30 | cumulative=30\n",
      "[Search] Page 2/50 -> request\n",
      "[Search] Page 2/50 -> found cards: 32\n",
      "[Search] Page 2/50 -> kept 13 | cumulative=43\n",
      "[Search] Page 3/50 -> request\n",
      "[Search] Page 3/50 -> found cards: 32\n",
      "[Search] Page 3/50 -> kept 6 | cumulative=49\n",
      "[Search] Page 4/50 -> request\n",
      "[Search] Page 4/50 -> found cards: 32\n",
      "[Search] Page 4/50 -> kept 2 | cumulative=51\n",
      "[Search] Page 5/50 -> request\n",
      "[Search] Page 5/50 -> found cards: 32\n",
      "[Search] Page 5/50 -> kept 4 | cumulative=55\n",
      "[Search] Page 6/50 -> request\n",
      "[Search] Page 6/50 -> found cards: 32\n",
      "[Search] Page 6/50 -> kept 6 | cumulative=61\n",
      "[Search] Page 7/50 -> request\n",
      "[Search] Page 7/50 -> found cards: 32\n",
      "[Search] Page 7/50 -> kept 7 | cumulative=68\n",
      "[Search] Page 8/50 -> request\n",
      "[Search] Page 8/50 -> found cards: 32\n",
      "[Search] Page 8/50 -> kept 2 | cumulative=70\n",
      "[Search] Page 9/50 -> request\n",
      "[Search] Page 9/50 -> found cards: 32\n",
      "[Search] Page 9/50 -> kept 1 | cumulative=71\n",
      "[Search] Page 10/50 -> request\n",
      "[Search] Page 10/50 -> found cards: 32\n",
      "[Search] Page 10/50 -> kept 1 | cumulative=72\n",
      "[Search] Page 11/50 -> request\n",
      "[Search] Page 11/50 -> found cards: 32\n",
      "[Search] Page 11/50 -> kept 2 | cumulative=74\n",
      "[Search] Page 12/50 -> request\n",
      "[Search] Page 12/50 -> found cards: 32\n",
      "[Search] Page 12/50 -> kept 1 | cumulative=75\n",
      "[Search] Page 13/50 -> request\n",
      "[Search] Page 13/50 -> found cards: 32\n",
      "[Search] Page 13/50 -> no keyword matches, stopping\n",
      "[Detail] Start detail scrape for 75 jobs\n",
      "[Detail] 10/75 (13.3%)\n",
      "[Detail] 20/75 (26.7%)\n",
      "[Detail] 30/75 (40.0%)\n",
      "[Detail] 40/75 (53.3%)\n",
      "[Detail] 50/75 (66.7%)\n",
      "[Detail] 60/75 (80.0%)\n",
      "[Detail] 70/75 (93.3%)\n",
      "[Detail] 75/75 (100.0%)\n",
      "[Done] Collected 75 rows for 'Data Engineer' job search in JobsDB\n"
     ]
    }
   ],
   "source": [
    "jobsdb_scraped_df = scrape_job_jobsdb(SEARCH_URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e244f85",
   "metadata": {},
   "source": [
    "## JobsDB Clean Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569878fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_jobsdb_salary_range(salary_text: str) -> tuple[str, str]:\n",
    "    text = clean_text(str(salary_text or \"\"))\n",
    "    if not text:\n",
    "        return \"\", \"\"\n",
    "\n",
    "    pattern = r\"฿?\\s*([\\d,]+)\\s*[-–]\\s*฿?\\s*([\\d,]+)\"\n",
    "    match = re.search(pattern, text)\n",
    "    if not match:\n",
    "        return \"\", \"\"\n",
    "\n",
    "    min_salary = match.group(1).replace(\",\", \"\")\n",
    "    max_salary = match.group(2).replace(\",\", \"\")\n",
    "    return min_salary, max_salary\n",
    "\n",
    "\n",
    "def parse_jobsdb_relative_posted_date(value: str, now_dt: datetime | None = None) -> str:\n",
    "    text = clean_text(str(value or \"\"))\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    current = now_dt or datetime.now()\n",
    "\n",
    "    hour_match = re.search(r\"(\\d+)\\s*ชั่วโมงที่ผ่านมา\", text)\n",
    "    if hour_match:\n",
    "        dt = current - timedelta(hours=int(hour_match.group(1)))\n",
    "        return dt.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "    day_match = re.search(r\"(\\d+)\\s*วันที่ผ่านมา\", text)\n",
    "    if day_match:\n",
    "        dt = current - timedelta(days=int(day_match.group(1)))\n",
    "        return dt.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "    minute_match = re.search(r\"(\\d+)\\s*นาทีที่ผ่านมา\", text)\n",
    "    if minute_match:\n",
    "        dt = current - timedelta(minutes=int(minute_match.group(1)))\n",
    "        return dt.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "    week_match = re.search(r\"(\\d+)\\s*สัปดาห์ที่ผ่านมา\", text)\n",
    "    if week_match:\n",
    "        dt = current - timedelta(days=7 * int(week_match.group(1)))\n",
    "        return dt.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "    month_match = re.search(r\"(\\d+)\\s*เดือนที่ผ่านมา\", text)\n",
    "    if month_match:\n",
    "        dt = current - timedelta(days=30 * int(month_match.group(1)))\n",
    "        return dt.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def clean_data_jobsdb(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    output = df.copy()\n",
    "\n",
    "    salary_pairs = output[\"salary\"].apply(extract_jobsdb_salary_range)\n",
    "    output[\"min_salary\"] = salary_pairs.apply(lambda pair: pair[0])\n",
    "    output[\"max_salary\"] = salary_pairs.apply(lambda pair: pair[1])\n",
    "\n",
    "    output[\"posted_date\"] = output[\"posted_date\"].apply(parse_jobsdb_relative_posted_date)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7695fe42",
   "metadata": {},
   "source": [
    "## JobsDB Clean & Export Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bb1eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobsdb_scraped_df = clean_data_jobsdb(jobsdb_scraped_df)\n",
    "jobsdb_scraped_df.to_csv(\"./Scraped_Each/jobsdb_jobs.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b829669d",
   "metadata": {},
   "source": [
    "## JOBBKK Scraper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9544321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (text or \"\")).strip()\n",
    "\n",
    "\n",
    "def normalize_for_match(text: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \" \", (text or \"\").lower()).strip()\n",
    "\n",
    "\n",
    "def normalize_for_skill_match(text: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \" \", (text or \"\").lower()).strip()\n",
    "\n",
    "def keyword_match_groups_from_query(search_keyword: str) -> list[list[str]]:\n",
    "\n",
    "    tokens = [token for token in normalize_for_match(search_keyword).split() if token]\n",
    "    groups = []\n",
    "\n",
    "    for token in tokens:\n",
    "        groups.append(KEY_VARIANTS.get(token, [token]))\n",
    "\n",
    "    return groups\n",
    "\n",
    "def title_matches_keyword(title: str, keyword_groups: list[list[str]]) -> bool:\n",
    "    if not keyword_groups:\n",
    "        return True\n",
    "    title_norm = normalize_for_match(title)\n",
    "    return all(any(variant in title_norm for variant in group) for group in keyword_groups)\n",
    "\n",
    "def variant_matches_text(variant: str, normalized_text: str) -> bool:\n",
    "    variant_norm = normalize_for_skill_match(variant)\n",
    "    if not variant_norm:\n",
    "        return False\n",
    "    pattern = rf\"(?<![a-z0-9]){re.escape(variant_norm).replace(r'\\\\ ', r'\\\\s+')}(?![a-z0-9])\"\n",
    "    return re.search(pattern, normalized_text) is not None\n",
    "\n",
    "\n",
    "def extract_skills(text: str) -> dict:\n",
    "    normalized_text = normalize_for_skill_match(text)\n",
    "    matched = []\n",
    "\n",
    "    for skill_name, variants in SKILLS.items():\n",
    "        if any(variant_matches_text(variant, normalized_text) for variant in variants):\n",
    "            matched.append(skill_name)\n",
    "\n",
    "    skill_flags = {f\"skill_{name}\": int(name in matched) for name in SKILLS}\n",
    "\n",
    "    return {\n",
    "        \"matched_skills\": \"|\".join(matched),\n",
    "        \"matched_skill_count\": len(matched),\n",
    "        **skill_flags,\n",
    "    }\n",
    "\n",
    "def extract_salary(text: str) -> str:\n",
    "    patterns = [\n",
    "        r\"\\d[\\d,\\s]*\\s*[-–]\\s*\\d[\\d,\\s]*\\s*บาท\",\n",
    "        r\"\\d[\\d,\\s]*\\s*บาท\",\n",
    "        r\"ตามตกลง|ตามประสบการณ์|ไม่ระบุเงินเดือน\",\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return clean_text(match.group(0))\n",
    "    return \"\"\n",
    "\n",
    "def update_page_in_search_url(url: str, page_num: int) -> str:\n",
    "    return re.sub(r\"/jobs/lists/\\d+/\", f\"/jobs/lists/{page_num}/\", url)\n",
    "\n",
    "\n",
    "def extract_keyword_from_url(url: str) -> str:\n",
    "    path = url.split(\"?\")[0]\n",
    "    parts = path.split(\",\")\n",
    "    if len(parts) >= 2:\n",
    "        return parts[1].replace(\"%20\", \" \")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def guess_province_name(location_text: str) -> str:\n",
    "    location_text = clean_text(location_text)\n",
    "    if not location_text:\n",
    "        return \"\"\n",
    "    for province in EN_TO_THAI_PROVINCE.values():\n",
    "        if province in location_text:\n",
    "            return province\n",
    "    return \"\"\n",
    "\n",
    "def parse_jobbkk_card(card, page_num: int, keyword: str) -> dict:\n",
    "    title_el = card.select_one(\".joblist-name-urgent a[href*='/jobs/detail']\")\n",
    "    company_el = card.select_one(\".joblist-company-name a\")\n",
    "    location_el = card.select_one(\".position-location span:last-child\")\n",
    "    salary_el = card.select_one(\".position-salary span:last-child\")\n",
    "    updated_el = card.select_one(\".joblist-updatetime-md-upper a\")\n",
    "\n",
    "    title = clean_text(title_el.get_text(\" \", strip=True) if title_el else \"\")\n",
    "    company = clean_text(company_el.get_text(\" \", strip=True) if company_el else \"\")\n",
    "    location = clean_text(location_el.get_text(\" \", strip=True) if location_el else \"\")\n",
    "    salary = clean_text(salary_el.get_text(\" \", strip=True) if salary_el else \"\")\n",
    "    posted_date = clean_text(updated_el.get(\"title\") if updated_el and updated_el.get(\"title\") else \"\")\n",
    "\n",
    "    href = title_el.get(\"href\", \"\") if title_el else \"\"\n",
    "    job_url = href if href.startswith(\"http\") else f\"https://jobbkk.com{href}\" if href else \"\"\n",
    "\n",
    "    if not job_url:\n",
    "        company_id = card.get(\"data-com-id\", \"\")\n",
    "        job_id = card.get(\"data-job-id\", \"\")\n",
    "        if company_id and job_id:\n",
    "            job_url = f\"https://jobbkk.com/jobs/detailurgent/{company_id}/{job_id}\"\n",
    "\n",
    "    raw_text = clean_text(card.get_text(\"\\n\", strip=True))\n",
    "    if not salary:\n",
    "        salary = extract_salary(raw_text)\n",
    "    if not posted_date and updated_el:\n",
    "        posted_date = clean_text(updated_el.get_text(\" \", strip=True))\n",
    "\n",
    "    province_name = guess_province_name(location)\n",
    "\n",
    "    return {\n",
    "        \"keyword\": keyword,\n",
    "        \"province_code\": \"\",\n",
    "        \"province_name\": province_name,\n",
    "        \"page\": page_num,\n",
    "        \"job_title\": title,\n",
    "        \"company\": company,\n",
    "        \"location\": location,\n",
    "        \"salary\": salary,\n",
    "        \"posted_date\": posted_date,\n",
    "        \"job_url\": job_url,\n",
    "        \"raw_text\": raw_text,\n",
    "    }\n",
    "\n",
    "def collect_list_items_text(container) -> str:\n",
    "    if container is None:\n",
    "        return \"\"\n",
    "    items = [clean_text(li.get_text(\" \", strip=True)) for li in container.select(\"li\")]\n",
    "    items = [item for item in items if item]\n",
    "    if items:\n",
    "        return \"\\n\".join(items)\n",
    "    return clean_text(container.get_text(\"\\n\", strip=True))\n",
    "\n",
    "def find_section_by_heading(root, heading_pattern: str):\n",
    "    heading_regex = re.compile(heading_pattern)\n",
    "    heading = root.find(\n",
    "        lambda tag: tag.name in [\"p\", \"span\", \"h2\", \"h3\", \"strong\"]\n",
    "        and heading_regex.search(clean_text(tag.get_text(\" \", strip=True)))\n",
    "    )\n",
    "    if not heading:\n",
    "        return None\n",
    "\n",
    "    for container in [heading.find_parent(\"section\"), heading.find_parent(\"div\")]:\n",
    "        if container and container.select(\"li\"):\n",
    "            return container\n",
    "\n",
    "    next_ul = heading.find_next(\"ul\")\n",
    "    if next_ul:\n",
    "        return next_ul\n",
    "\n",
    "    return heading.find_parent(\"section\") or heading.find_parent(\"div\")\n",
    "\n",
    "def extract_jobbkk_detail(job_url: str, headers: dict) -> dict:\n",
    "    base_detail = {\n",
    "        \"job_detail_full_text\": \"\",\n",
    "        \"matched_skills\": \"\",\n",
    "        \"matched_skill_count\": 0,\n",
    "        **{column: 0 for column in SKILL_COLUMNS},\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(job_url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "    except Exception:\n",
    "        return base_detail\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    detail_root = soup.select_one(\"article.row\") or soup\n",
    "    job_detail_full_text = clean_text(detail_root.get_text(\"\\n\", strip=True))\n",
    "\n",
    "    skill_info = extract_skills(job_detail_full_text)\n",
    "\n",
    "    return {\n",
    "        \"job_detail_full_text\": job_detail_full_text,\n",
    "        **skill_info,\n",
    "    }\n",
    "\n",
    "def scrape_job_jobbkk(\n",
    "    search_url: str = \"\",\n",
    "    max_pages: int = 50,\n",
    "    sleep_seconds: float = 0.5,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    collected_frames = []\n",
    "    try:\n",
    "        for job in SEARCH_URLS[\"JOBBKK\"]:\n",
    "            keyword = job[0]\n",
    "            search_url = job[1]\n",
    "            keyword_groups = keyword_match_groups_from_query(keyword)\n",
    "\n",
    "            all_rows = []\n",
    "            seen_urls = set()\n",
    "\n",
    "            print(f\"[Search] Starting JobBKK crawl: max_pages={max_pages}\")\n",
    "\n",
    "            for page_num in range(1, max_pages + 1):\n",
    "                page_url = update_page_in_search_url(search_url, page_num)\n",
    "                print(f\"[Search] Page {page_num}/{max_pages} -> request\")\n",
    "\n",
    "                response = requests.get(page_url, headers=headers, timeout=30)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                cards = soup.select(\"div.joblist-pos.jobbkk-list-company\")\n",
    "                print(f\"[Search] Page {page_num}/{max_pages} -> found cards: {len(cards)}\")\n",
    "\n",
    "                if not cards:\n",
    "                    print(f\"[Search] Page {page_num}/{max_pages} -> no cards, stopping\")\n",
    "                    break\n",
    "\n",
    "                page_rows = []\n",
    "                for card in cards:\n",
    "                    row = parse_jobbkk_card(card, page_num=page_num, keyword=keyword)\n",
    "\n",
    "                    if not row[\"job_title\"] or not row[\"job_url\"]:\n",
    "                        continue\n",
    "                    if not title_matches_keyword(row[\"job_title\"], keyword_groups):\n",
    "                        continue\n",
    "                    if row[\"job_url\"] in seen_urls:\n",
    "                        continue\n",
    "\n",
    "                    seen_urls.add(row[\"job_url\"])\n",
    "                    page_rows.append(row)\n",
    "\n",
    "                if not page_rows:\n",
    "                    print(f\"[Search] Page {page_num}/{max_pages} -> no keyword matches, stopping\")\n",
    "                    break\n",
    "\n",
    "                all_rows.extend(page_rows)\n",
    "                print(f\"[Search] Page {page_num}/{max_pages} -> kept {len(page_rows)} | cumulative={len(all_rows)}\")\n",
    "\n",
    "                if sleep_seconds > 0:\n",
    "                    time.sleep(sleep_seconds)\n",
    "\n",
    "            total_details = len(all_rows)\n",
    "            print(f\"[Detail] Starting detail extraction for {total_details} jobs\")\n",
    "\n",
    "            for index, row in enumerate(all_rows, start=1):\n",
    "                detail_info = extract_jobbkk_detail(row[\"job_url\"], headers=headers)\n",
    "                row.update(detail_info)\n",
    "\n",
    "                if total_details <= 50 or index % 10 == 0 or index == total_details:\n",
    "                    percent = (index / total_details) * 100 if total_details else 100\n",
    "                    print(f\"[Detail] {index}/{total_details} ({percent:.1f}%)\")\n",
    "\n",
    "                if sleep_seconds > 0:\n",
    "                    time.sleep(sleep_seconds)\n",
    "\n",
    "            job_df = pd.DataFrame(all_rows)\n",
    "\n",
    "            if job_df.empty:\n",
    "                continue\n",
    "\n",
    "            job_df[\"domain\"] = \"JOBBKK\"\n",
    "            job_df[\"min_salary\"] = None\n",
    "            job_df[\"max_salary\"] = None            \n",
    "\n",
    "            ordered_cols = [\n",
    "            \"domain\",\n",
    "            \"keyword\",\n",
    "            \"province_name\",\n",
    "            \"job_title\",\n",
    "            \"company\",\n",
    "            \"location\",\n",
    "            \"salary\",\n",
    "            \"min_salary\",\n",
    "            \"max_salary\",\n",
    "            \"posted_date\",\n",
    "            \"job_url\",\n",
    "            \"matched_skills\",\n",
    "            \"matched_skill_count\",\n",
    "            *SKILL_COLUMNS,\n",
    "            ]\n",
    "\n",
    "            for column in ordered_cols:\n",
    "                if column not in job_df.columns:\n",
    "                    job_df[column] = \"\" if column not in {\"matched_skill_count\", *SKILL_COLUMNS} else 0\n",
    "\n",
    "            job_df = job_df[ordered_cols].drop_duplicates(subset=[\"job_url\"])\n",
    "            collected_frames.append(job_df)\n",
    "            print(f\"[Done] Collected {len(job_df)} rows for '{keyword}' job search in JobBKK\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred on JobBKK scraping: {e}\")\n",
    "        print(\"Skipping JobBKK and returning collected data so far.\")\n",
    "    \n",
    "    if not collected_frames:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    final_df = pd.concat(collected_frames, ignore_index=True)\n",
    "    final_df = final_df.drop_duplicates(subset=[\"job_url\"])\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf511c",
   "metadata": {},
   "source": [
    "## JOBBKK Scraper Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "817238ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search] Starting JobBKK crawl: max_pages=50\n",
      "[Search] Page 1/50 -> request\n",
      "[Search] Page 1/50 -> found cards: 15\n",
      "[Search] Page 1/50 -> kept 2 | cumulative=2\n",
      "[Search] Page 2/50 -> request\n",
      "[Search] Page 2/50 -> found cards: 15\n",
      "[Search] Page 2/50 -> no keyword matches, stopping\n",
      "[Detail] Starting detail extraction for 2 jobs\n",
      "[Detail] 1/2 (50.0%)\n",
      "[Detail] 2/2 (100.0%)\n",
      "[Done] Collected 2 rows for 'Data Scientist' job search in JobBKK\n",
      "[Search] Starting JobBKK crawl: max_pages=50\n",
      "[Search] Page 1/50 -> request\n",
      "[Search] Page 1/50 -> found cards: 15\n",
      "[Search] Page 1/50 -> kept 8 | cumulative=8\n",
      "[Search] Page 2/50 -> request\n",
      "[Search] Page 2/50 -> found cards: 15\n",
      "[Search] Page 2/50 -> kept 5 | cumulative=13\n",
      "[Search] Page 3/50 -> request\n",
      "[Search] Page 3/50 -> found cards: 15\n",
      "[Search] Page 3/50 -> kept 1 | cumulative=14\n",
      "[Search] Page 4/50 -> request\n",
      "[Search] Page 4/50 -> found cards: 15\n",
      "[Search] Page 4/50 -> kept 4 | cumulative=18\n",
      "[Search] Page 5/50 -> request\n",
      "[Search] Page 5/50 -> found cards: 15\n",
      "[Search] Page 5/50 -> kept 2 | cumulative=20\n",
      "[Search] Page 6/50 -> request\n",
      "[Search] Page 6/50 -> found cards: 15\n",
      "[Search] Page 6/50 -> kept 4 | cumulative=24\n",
      "[Search] Page 7/50 -> request\n",
      "[Search] Page 7/50 -> found cards: 15\n",
      "[Search] Page 7/50 -> kept 2 | cumulative=26\n",
      "[Search] Page 8/50 -> request\n",
      "[Search] Page 8/50 -> found cards: 15\n",
      "[Search] Page 8/50 -> kept 3 | cumulative=29\n",
      "[Search] Page 9/50 -> request\n",
      "[Search] Page 9/50 -> found cards: 15\n",
      "[Search] Page 9/50 -> kept 3 | cumulative=32\n",
      "[Search] Page 10/50 -> request\n",
      "[Search] Page 10/50 -> found cards: 15\n",
      "[Search] Page 10/50 -> no keyword matches, stopping\n",
      "[Detail] Starting detail extraction for 32 jobs\n",
      "[Detail] 1/32 (3.1%)\n",
      "[Detail] 2/32 (6.2%)\n",
      "[Detail] 3/32 (9.4%)\n",
      "[Detail] 4/32 (12.5%)\n",
      "[Detail] 5/32 (15.6%)\n",
      "[Detail] 6/32 (18.8%)\n",
      "[Detail] 7/32 (21.9%)\n",
      "[Detail] 8/32 (25.0%)\n",
      "[Detail] 9/32 (28.1%)\n",
      "[Detail] 10/32 (31.2%)\n",
      "[Detail] 11/32 (34.4%)\n",
      "[Detail] 12/32 (37.5%)\n",
      "[Detail] 13/32 (40.6%)\n",
      "[Detail] 14/32 (43.8%)\n",
      "[Detail] 15/32 (46.9%)\n",
      "[Detail] 16/32 (50.0%)\n",
      "[Detail] 17/32 (53.1%)\n",
      "[Detail] 18/32 (56.2%)\n",
      "[Detail] 19/32 (59.4%)\n",
      "[Detail] 20/32 (62.5%)\n",
      "[Detail] 21/32 (65.6%)\n",
      "[Detail] 22/32 (68.8%)\n",
      "[Detail] 23/32 (71.9%)\n",
      "[Detail] 24/32 (75.0%)\n",
      "[Detail] 25/32 (78.1%)\n",
      "[Detail] 26/32 (81.2%)\n",
      "[Detail] 27/32 (84.4%)\n",
      "[Detail] 28/32 (87.5%)\n",
      "[Detail] 29/32 (90.6%)\n",
      "[Detail] 30/32 (93.8%)\n",
      "[Detail] 31/32 (96.9%)\n",
      "[Detail] 32/32 (100.0%)\n",
      "[Done] Collected 32 rows for 'Data Analyst' job search in JobBKK\n",
      "[Search] Starting JobBKK crawl: max_pages=50\n",
      "[Search] Page 1/50 -> request\n",
      "[Search] Page 1/50 -> found cards: 15\n",
      "[Search] Page 1/50 -> kept 6 | cumulative=6\n",
      "[Search] Page 2/50 -> request\n",
      "[Search] Page 2/50 -> found cards: 15\n",
      "[Search] Page 2/50 -> kept 6 | cumulative=12\n",
      "[Search] Page 3/50 -> request\n",
      "[Search] Page 3/50 -> found cards: 15\n",
      "[Search] Page 3/50 -> kept 2 | cumulative=14\n",
      "[Search] Page 4/50 -> request\n",
      "[Search] Page 4/50 -> found cards: 15\n",
      "[Search] Page 4/50 -> no keyword matches, stopping\n",
      "[Detail] Starting detail extraction for 14 jobs\n",
      "[Detail] 1/14 (7.1%)\n",
      "[Detail] 2/14 (14.3%)\n",
      "[Detail] 3/14 (21.4%)\n",
      "[Detail] 4/14 (28.6%)\n",
      "[Detail] 5/14 (35.7%)\n",
      "[Detail] 6/14 (42.9%)\n",
      "[Detail] 7/14 (50.0%)\n",
      "[Detail] 8/14 (57.1%)\n",
      "[Detail] 9/14 (64.3%)\n",
      "[Detail] 10/14 (71.4%)\n",
      "[Detail] 11/14 (78.6%)\n",
      "[Detail] 12/14 (85.7%)\n",
      "[Detail] 13/14 (92.9%)\n",
      "[Detail] 14/14 (100.0%)\n",
      "[Done] Collected 14 rows for 'Data Engineer' job search in JobBKK\n"
     ]
    }
   ],
   "source": [
    "jobbkk_scraped_df = scrape_job_jobbkk(SEARCH_URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a1308",
   "metadata": {},
   "source": [
    "## JOBBKK Clean Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "866bbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_jobbkk_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    salary_parts = out[\"salary\"].fillna(\"\").astype(str).str.extract(\n",
    "        r\"(?P<min>\\d{1,3}(?:,\\d{3})*)\\s*[-–]\\s*(?P<max>\\d{1,3}(?:,\\d{3})*)\\s*บาท?\",\n",
    "        expand=True,\n",
    "    )\n",
    "    out[\"min_salary\"] = salary_parts[\"min\"].fillna(\"\").str.replace(\",\", \"\", regex=False)\n",
    "    out[\"max_salary\"] = salary_parts[\"max\"].fillna(\"\").str.replace(\",\", \"\", regex=False)\n",
    "\n",
    "    parsed_dates = pd.to_datetime(\n",
    "        out[\"posted_date\"].fillna(\"\").astype(str),\n",
    "        format=\"%d/%m/%Y %H:%M\",\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    out[\"posted_date\"] = parsed_dates.dt.strftime(\"%m/%d/%Y\").fillna(\"\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb46d3",
   "metadata": {},
   "source": [
    "## JOBBKK Clean & Export Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b969444",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobbkk_scraped_df = clean_jobbkk_data(jobbkk_scraped_df)\n",
    "jobbkk_scraped_df.to_csv(\"./Scraped_Each/jobbkk_jobs.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
