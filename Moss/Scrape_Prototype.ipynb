{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f783e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import parse_qs, urlencode, urlparse, urlunparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3749c2d",
   "metadata": {},
   "source": [
    "## Initiate basic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c545f229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search URLs:\n",
      "JobThai:\n",
      "  ['Data Scientist', 'https://www.jobthai.com/th/jobs?keyword=Data%20Scientist&page=1&orderBy=RELEVANCE_SEARCH']\n",
      "JobsDB:\n",
      "  ['Data Scientist', 'https://th.jobsdb.com/th/Data-Scientist-jobs']\n",
      "JOBBKK:\n",
      "  ['Data Scientist', 'https://jobbkk.com/jobs/lists/1/หางาน,Data%20Scientist,ทุกจังหวัด,ทั้งหมด.html?keyword_type=3&sort=4']\n"
     ]
    }
   ],
   "source": [
    "# JOBS_LIST = ['Data Scientist', 'Data Analyst', 'Data Engineer']\n",
    "JOBS_LIST = ['Data Scientist']\n",
    "SKILLS = {\n",
    "    # ---------------- Core Programming ----------------\n",
    "    \"python\": [\"python\"],\n",
    "    \"r\": [\" r \", \" r,\", \" r\\n\", \" r/\"],\n",
    "    \"java\": [\"java\"],\n",
    "    \"scala\": [\"scala\"],\n",
    "    \"c++\": [\"c++\"],\n",
    "\n",
    "    # ---------------- SQL & Databases ----------------\n",
    "    \"sql & database\": [\" sql \", \"mysql\", \"postgres\", \"postgresql\", \"oracle\", \"sql server\", \"mssql\", \"sqlite\"],\n",
    "    \"mongodb\": [\"mongodb\", \"mongo\"],\n",
    "    \"elasticsearch\": [\"elasticsearch\", \"elastic search\"],\n",
    "\n",
    "    # ---------------- Data Libraries ----------------\n",
    "    \"pandas\": [\"pandas\"],\n",
    "    \"numpy\": [\"numpy\"],\n",
    "    \"scipy\": [\"scipy\"],\n",
    "    \"sklearn\": [\"scikit-learn\", \"sklearn\"],\n",
    "\n",
    "    # ---------------- Machine Learning ----------------\n",
    "    \"machine_learning\": [\n",
    "        \"machine learning\", \"supervised\", \"unsupervised\",\n",
    "        \"random forest\", \"xgboost\", \"lightgbm\", \"catboost\"\n",
    "    ],\n",
    "\n",
    "    # ---------------- Deep Learning ----------------\n",
    "    \"deep_learning\": [\n",
    "        \"deep learning\", \"neural network\", \"cnn\", \"rnn\", \"lstm\", \"transformer\"\n",
    "    ],\n",
    "\n",
    "    # ---------------- GenAI / LLM ----------------\n",
    "    \"llm\": [\"llm\", \"large language model\"],\n",
    "    \"rag\": [\"rag\", \"retrieval augmented generation\"],\n",
    "    \"langchain\": [\"langchain\"],\n",
    "    \"openai\": [\"openai\"],\n",
    "    \"huggingface\": [\"huggingface\"],\n",
    "    \"prompt_engineering\": [\"prompt engineering\"],\n",
    "    \"vector_db\": [\"vector database\", \"pinecone\", \"faiss\", \"weaviate\", \"milvus\"],\n",
    "\n",
    "    # ---------------- Visualization / BI ----------------\n",
    "    \"excel\": [\"excel\", \"vlookup\", \"pivot table\", \"power query\"],\n",
    "    \"powerbi\": [\"power bi\", \"powerbi\", \"dax\"],\n",
    "    \"tableau\": [\"tableau\"],\n",
    "    \"matplotlib\": [\"matplotlib\"],\n",
    "    \"seaborn\": [\"seaborn\"],\n",
    "    \"plotly\": [\"plotly\"],\n",
    "\n",
    "    # ---------------- Big Data ----------------\n",
    "    \"spark\": [\"spark\", \"pyspark\"],\n",
    "    \"hadoop\": [\"hadoop\"],\n",
    "    \"kafka\": [\"kafka\"],\n",
    "\n",
    "    # ---------------- Cloud ----------------\n",
    "    \"aws\": [\"aws\", \"amazon web services\", \"s3\", \"redshift\", \"athena\", \"glue\", \"lambda\"],\n",
    "    \"gcp\": [\"gcp\", \"google cloud\", \"bigquery\", \"cloud storage\"],\n",
    "    \"azure\": [\"azure\", \"synapse\", \"databricks\"],\n",
    "\n",
    "    # ---------------- Data Engineering ----------------\n",
    "    \"etl\": [\"etl\", \"elt\", \"data pipeline\"],\n",
    "    \"airflow\": [\"airflow\"],\n",
    "\n",
    "    # ---------------- MLOps / Deployment ----------------\n",
    "    \"docker\": [\"docker\"],\n",
    "    \"kubernetes\": [\"kubernetes\", \"k8s\"],\n",
    "    \"mlflow\": [\"mlflow\"],\n",
    "    \"fastapi\": [\"fastapi\"],\n",
    "    \"flask\": [\"flask\"],\n",
    "    \"streamlit\": [\"streamlit\"],\n",
    "\n",
    "    # ---------------- Statistics ----------------\n",
    "    \"statistics\": [\n",
    "        \"statistics\", \"statistical\", \"hypothesis testing\",\n",
    "        \"regression\", \"anova\", \"probability\"\n",
    "    ],\n",
    "\n",
    "    # ---------------- Version Control ----------------\n",
    "    \"git\": [\"git\", \"github\", \"gitlab\"],\n",
    "\n",
    "    # ---------------- APIs ----------------\n",
    "    \"api\": [\"api\", \"rest api\"],\n",
    "\n",
    "    # ---------------- Linux ----------------\n",
    "    \"linux\": [\"linux\", \"unix\"],    \n",
    "}\n",
    "\n",
    "SEARCH_URLS = {\n",
    "    \"JobThai\": [[job_title,f\"https://www.jobthai.com/th/jobs?keyword={job_title}&page=1&orderBy=RELEVANCE_SEARCH\".replace(\" \", \"%20\")] for job_title in JOBS_LIST],\n",
    "    \"JobsDB\": [[job_title,f\"https://th.jobsdb.com/th/{job_title}-jobs\".replace(\" \", \"-\")] for job_title in JOBS_LIST],\n",
    "    \"JOBBKK\": [[job_title,f\"https://jobbkk.com/jobs/lists/1/หางาน,{job_title},ทุกจังหวัด,ทั้งหมด.html?keyword_type=3&sort=4\".replace(\" \", \"%20\")] for job_title in JOBS_LIST],\n",
    "}\n",
    "\n",
    "KEY_VARIANTS = {\n",
    "    \"data\": [\"data\"],\n",
    "    \"scientist\": [\"scientist\", \"science\", \"scien\", \"scient\"],\n",
    "    \"engineer\": [\"engineer\", \"engineering\", \"eng\"],\n",
    "    \"analyst\": [\"analyst\", \"analytics\", \"analysis\"],\n",
    "    \"developer\": [\"developer\", \"development\", \"dev\"],\n",
    "}\n",
    "\n",
    "SKILL_COLUMNS = [f\"skill_{name}\" for name in SKILLS]\n",
    "\n",
    "# For Debugging Start\n",
    "print(\"Search URLs:\")\n",
    "for platform, urls in SEARCH_URLS.items():\n",
    "    print(f\"{platform}:\")\n",
    "    for url in urls:\n",
    "        print(f\"  {url}\")\n",
    "# For Debugging End     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9b526",
   "metadata": {},
   "source": [
    "## JobThai Scraper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1525dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_province_code(value) -> str:\n",
    "    text = str(value).strip()\n",
    "    if text.isdigit():\n",
    "        number = int(text)\n",
    "        if number <= 0:\n",
    "            raise ValueError(f\"Province must be positive, got: {value}\")\n",
    "        return f\"{number:02d}\"\n",
    "    raise ValueError(f\"Invalid province code: {value}\")\n",
    "\n",
    "\n",
    "def normalize_for_match(text: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \" \", text.lower()).strip()\n",
    "\n",
    "\n",
    "def normalize_for_skill_match(text: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \" \", text.lower()).strip()\n",
    "\n",
    "\n",
    "def keyword_match_groups_from_query(keyword: str) -> list[list[str]]:\n",
    "    tokens = [token for token in normalize_for_match(keyword).split() if token]\n",
    "    groups = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in KEY_VARIANTS:\n",
    "            groups.append(KEY_VARIANTS[token])\n",
    "        else:\n",
    "            groups.append([token])\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def title_matches_keyword(title: str, keyword_groups: list[list[str]]) -> bool:\n",
    "    if not keyword_groups:\n",
    "        return True\n",
    "\n",
    "    title_norm = normalize_for_match(title)\n",
    "    search_from = 0\n",
    "\n",
    "    for group in keyword_groups:\n",
    "        best_pos = None\n",
    "        best_variant = \"\"\n",
    "\n",
    "        for variant in group:\n",
    "            variant_norm = normalize_for_match(variant)\n",
    "            if not variant_norm:\n",
    "                continue\n",
    "\n",
    "            pos = title_norm.find(variant_norm, search_from)\n",
    "            if pos != -1 and (best_pos is None or pos < best_pos):\n",
    "                best_pos = pos\n",
    "                best_variant = variant_norm\n",
    "\n",
    "        if best_pos is None:\n",
    "            return False\n",
    "\n",
    "        search_from = best_pos + len(best_variant)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def variant_matches_text(variant: str, normalized_text: str) -> bool:\n",
    "    variant_norm = normalize_for_skill_match(variant)\n",
    "    if not variant_norm:\n",
    "        return False\n",
    "    pattern = rf\"(?<![a-z0-9]){re.escape(variant_norm).replace(r'\\\\ ', r'\\\\s+')}(?![a-z0-9])\"\n",
    "    return re.search(pattern, normalized_text) is not None\n",
    "\n",
    "\n",
    "def extract_skills(text: str) -> dict:\n",
    "    normalized_text = normalize_for_skill_match(text)\n",
    "    matched = []\n",
    "\n",
    "    for skill_name, variants in SKILLS.items():\n",
    "        if any(variant_matches_text(variant, normalized_text) for variant in variants):\n",
    "            matched.append(skill_name)\n",
    "\n",
    "    skill_flags = {f\"skill_{name}\": int(name in matched) for name in SKILLS}\n",
    "\n",
    "    return {\n",
    "        \"matched_skills\": \"|\".join(matched),\n",
    "        \"matched_skill_count\": len(matched),\n",
    "        **skill_flags,\n",
    "    }\n",
    "\n",
    "\n",
    "def normalize_jobthai_detail_url(job_url: str) -> str:\n",
    "    if not job_url:\n",
    "        return \"\"\n",
    "\n",
    "    parsed = urlparse(job_url)\n",
    "    path = parsed.path\n",
    "\n",
    "    path = path.replace(\"/th/company/job/\", \"/th/job/\")\n",
    "    path = path.replace(\"/company/job/\", \"/job/\")\n",
    "\n",
    "    return urlunparse((parsed.scheme, parsed.netloc, path, \"\", \"\", \"\"))\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    return \" \".join((text or \"\").split())\n",
    "\n",
    "\n",
    "def extract_salary(text: str) -> str:\n",
    "    patterns = [\n",
    "        r\"\\d[\\d,\\s]*\\s*-\\s*\\d[\\d,\\s]*\\s*บาท\",\n",
    "        r\"\\d[\\d,\\s]*\\s*บาท\",\n",
    "        r\"ตามโครงสร้างบริษัทฯ\",\n",
    "        r\"ตามประสบการณ์\",\n",
    "        r\"ตามตกลง\",\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return clean_text(match.group(0))\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_posted_date(text: str) -> str:\n",
    "    match = re.search(r\"\\b\\d{1,2}\\s+[ก-๙A-Za-z\\.]+\\s+\\d{2}\\b\", text)\n",
    "    return clean_text(match.group(0)) if match else \"\"\n",
    "\n",
    "\n",
    "def pick_text(parent, selectors: list[str]) -> str:\n",
    "    for selector in selectors:\n",
    "        element = parent.select_one(selector)\n",
    "        if element:\n",
    "            text = clean_text(element.get_text(\" \", strip=True))\n",
    "            if text:\n",
    "                return text\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def guess_location(lines: list[str], title: str, company: str, salary: str) -> str:\n",
    "    priority_keywords = [\"เขต\", \"กรุงเทพ\", \"จังหวัด\", \"อำเภอ\", \"อ.\", \"ต.\"]\n",
    "    transit_keywords = [\"BTS\", \"MRT\", \"SRT\", \"BRT\", \"Airport Rail Link\"]\n",
    "\n",
    "    for line in lines:\n",
    "        if line in {title, company, salary}:\n",
    "            continue\n",
    "        if any(keyword in line for keyword in priority_keywords):\n",
    "            return line\n",
    "\n",
    "    for line in lines:\n",
    "        if line in {title, company, salary}:\n",
    "            continue\n",
    "        if any(keyword in line for keyword in transit_keywords):\n",
    "            return line\n",
    "\n",
    "    if salary and salary in lines:\n",
    "        salary_idx = lines.index(salary)\n",
    "        for idx in range(salary_idx - 1, -1, -1):\n",
    "            candidate = lines[idx]\n",
    "            if candidate not in {title, company}:\n",
    "                return candidate\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_card_from_title(title_node, page_num: int, keyword: str) -> dict:\n",
    "    title = clean_text(title_node.get_text(\" \", strip=True))\n",
    "\n",
    "    anchor = title_node.find_parent(\"a\", href=True)\n",
    "    href = anchor.get(\"href\", \"\") if anchor else \"\"\n",
    "    job_url = href if href.startswith(\"http\") else f\"https://www.jobthai.com{href}\"\n",
    "    job_url = normalize_jobthai_detail_url(job_url)\n",
    "\n",
    "    card = anchor if anchor is not None else title_node\n",
    "\n",
    "    company = pick_text(card, [\n",
    "        'span[id^=\"job-list-company-name-\"]',\n",
    "        'h2.ohgq7e-0.enAWkF',\n",
    "    ])\n",
    "\n",
    "    location = pick_text(card, [\n",
    "        \"h3#location-text\",\n",
    "        \"h3.location-text\",\n",
    "    ])\n",
    "\n",
    "    salary = pick_text(card, [\n",
    "        \"span.salary-text\",\n",
    "        \"div.msklqa-20\",\n",
    "        \"div.msklqa-17\",\n",
    "    ])\n",
    "\n",
    "    posted_date = pick_text(card, [\n",
    "        \"span.msklqa-9\",\n",
    "    ])\n",
    "\n",
    "    raw_lines = [clean_text(x) for x in card.get_text(\"\\n\", strip=True).splitlines() if clean_text(x)]\n",
    "    raw_text = clean_text(\" \".join(raw_lines))\n",
    "\n",
    "    if not salary:\n",
    "        salary = extract_salary(raw_text)\n",
    "    if not posted_date:\n",
    "        posted_date = extract_posted_date(raw_text)\n",
    "    if not location:\n",
    "        location = guess_location(raw_lines, title=title, company=company, salary=salary)\n",
    "\n",
    "    return {\n",
    "        \"keyword\": keyword,\n",
    "        \"page\": page_num,\n",
    "        \"job_title\": title,\n",
    "        \"company\": company,\n",
    "        \"location\": location,\n",
    "        \"salary\": salary,\n",
    "        \"posted_date\": posted_date,\n",
    "        \"job_url\": job_url,\n",
    "        \"raw_text\": raw_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_detail_from_job_page(job_url: str, headers: dict) -> dict:\n",
    "    base_detail = {\n",
    "        \"province_code\": \"\",\n",
    "        \"province_name\": \"\",\n",
    "        \"job_detail_text\": \"\",\n",
    "        \"job_qualification_text\": \"\",\n",
    "        \"matched_skills\": \"\",\n",
    "        \"matched_skill_count\": 0,\n",
    "        **{column: 0 for column in SKILL_COLUMNS},\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(job_url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "    except Exception:\n",
    "        return base_detail\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    province_code = \"\"\n",
    "    province_name = \"\"\n",
    "    for anchor in soup.select('a[href*=\"province=\"]'):\n",
    "        tag = anchor.select_one('h3[id^=\"job-detail-tag-\"]')\n",
    "        if not tag:\n",
    "            continue\n",
    "\n",
    "        href = anchor.get(\"href\", \"\")\n",
    "        name = clean_text(tag.get_text(\" \", strip=True))\n",
    "        if not href or not name:\n",
    "            continue\n",
    "\n",
    "        province_value = parse_qs(urlparse(href).query).get(\"province\", [\"\"])[0]\n",
    "        if not province_value or not province_value.isdigit():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            province_code = normalize_province_code(province_value)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        province_name = name\n",
    "        break\n",
    "\n",
    "    jd_node = soup.select_one(\"span#job-detail\")\n",
    "    job_detail_text = clean_text(jd_node.get_text(\"\\n\", strip=True)) if jd_node else \"\"\n",
    "\n",
    "    qualification_node = soup.select_one(\"#job-properties-wrapper\")\n",
    "    job_qualification_text = clean_text(qualification_node.get_text(\" \", strip=True)) if qualification_node else \"\"\n",
    "\n",
    "    combined_text = \" \".join([text for text in [job_detail_text, job_qualification_text] if text])\n",
    "    skill_info = extract_skills(combined_text)\n",
    "\n",
    "    return {\n",
    "        \"province_code\": province_code,\n",
    "        \"province_name\": province_name,\n",
    "        \"job_detail_text\": job_detail_text,\n",
    "        \"job_qualification_text\": job_qualification_text,\n",
    "        **skill_info,\n",
    "    }\n",
    "\n",
    "\n",
    "def scrape_job_jobthai(\n",
    "    SEARCH_URLS: dict[str, list[str]],\n",
    "    SLEEP_SEC: float = 0.5,\n",
    ") -> pd.DataFrame:\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"th-TH,th;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    }\n",
    "\n",
    "    collected_frames = []\n",
    "\n",
    "    try:\n",
    "        for job in SEARCH_URLS[\"JobThai\"]:\n",
    "            keyword = job[0]\n",
    "            search_url = job[1]\n",
    "\n",
    "            keyword_groups = keyword_match_groups_from_query(keyword)\n",
    "            print(f\"Scraping JobThai for '{keyword}'\")\n",
    "\n",
    "            all_rows = []\n",
    "            seen_urls = set()\n",
    "\n",
    "            for page_no in range(1, 50):\n",
    "                page_url = search_url.replace(\"page=1\", f\"page={page_no}\")\n",
    "                print(f\"\\tFetching page {page_no}\")\n",
    "\n",
    "                response = requests.get(page_url, headers=headers, timeout=30)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                if \"nodata=true\" in response.url.lower():\n",
    "                    print(\"No data found for this keyword.\")\n",
    "                    break\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                title_cards_html = soup.select('h2[id^=\"job-card-item-\"]')\n",
    "\n",
    "                page_rows = []\n",
    "                for title_card_html in title_cards_html:\n",
    "                    row = parse_card_from_title(\n",
    "                        title_card_html,\n",
    "                        page_num=page_no,\n",
    "                        keyword=keyword,\n",
    "                    )\n",
    "\n",
    "                    if not row[\"job_url\"]:\n",
    "                        continue\n",
    "                    if not title_matches_keyword(row[\"job_title\"], keyword_groups):\n",
    "                        continue\n",
    "                    if row[\"job_url\"] in seen_urls:\n",
    "                        continue\n",
    "\n",
    "                    seen_urls.add(row[\"job_url\"])\n",
    "                    page_rows.append(row)\n",
    "\n",
    "                if not page_rows:\n",
    "                    break\n",
    "\n",
    "                all_rows.extend(page_rows)\n",
    "\n",
    "                if SLEEP_SEC > 0:\n",
    "                    time.sleep(SLEEP_SEC)\n",
    "\n",
    "            total_details = len(all_rows)\n",
    "            print(f\"[Detail] Starting detail extraction for {total_details} jobs\")\n",
    "\n",
    "            for row in all_rows:\n",
    "                detail_info = extract_detail_from_job_page(row[\"job_url\"], headers=headers)\n",
    "                row.update(detail_info)\n",
    "\n",
    "                if SLEEP_SEC > 0:\n",
    "                    time.sleep(SLEEP_SEC)\n",
    "\n",
    "            job_df = pd.DataFrame(all_rows)\n",
    "            if job_df.empty:\n",
    "                continue\n",
    "            job_df[\"domain\"] = \"JobThai\"\n",
    "            job_df[\"min_salary\"] = None\n",
    "            job_df[\"max_salary\"] = None\n",
    "\n",
    "            ordered_columns = [\n",
    "                \"domain\",\n",
    "                \"keyword\",\n",
    "                \"province_name\",\n",
    "                \"job_title\",\n",
    "                \"company\",\n",
    "                \"location\",\n",
    "                \"salary\",\n",
    "                \"min_salary\",\n",
    "                \"max_salary\",\n",
    "                \"posted_date\",\n",
    "                \"job_url\",\n",
    "                \"matched_skills\",\n",
    "                \"matched_skill_count\",\n",
    "                *SKILL_COLUMNS,\n",
    "            ]\n",
    "            for column in ordered_columns:\n",
    "                if column not in job_df.columns:\n",
    "                    job_df[column] = \"\" if column not in {\"matched_skill_count\", *SKILL_COLUMNS} else 0\n",
    "\n",
    "            job_df = job_df[ordered_columns].drop_duplicates(subset=[\"job_url\"])\n",
    "            collected_frames.append(job_df)\n",
    "            print(f\"[Done] Collected {len(job_df)} rows for '{keyword}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred on JobThai scraping: {e}\")\n",
    "        print(\"Skipping JobThai and returning collected data so far.\")\n",
    "\n",
    "    if not collected_frames:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_df = pd.concat(collected_frames, ignore_index=True)\n",
    "    final_df = final_df.drop_duplicates(subset=[\"job_url\"])\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abcb61",
   "metadata": {},
   "source": [
    "## JobThai Scraper Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fcb800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping JobThai for 'Data Scientist'\n",
      "\tFetching page 1\n",
      "\tFetching page 2\n",
      "[Detail] Starting detail extraction for 7 jobs\n",
      "[Done] Collected 7 rows for 'Data Scientist'\n"
     ]
    }
   ],
   "source": [
    "jobs_df = scrape_job_jobthai(SEARCH_URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf7533",
   "metadata": {},
   "source": [
    "## Clean Data JobThai Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff456f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "THAI_MONTHS = {\n",
    "    \"ม.ค.\": 1,\n",
    "    \"ก.พ.\": 2,\n",
    "    \"มี.ค.\": 3,\n",
    "    \"เม.ย.\": 4,\n",
    "    \"พ.ค.\": 5,\n",
    "    \"มิ.ย.\": 6,\n",
    "    \"ก.ค.\": 7,\n",
    "    \"ส.ค.\": 8,\n",
    "    \"ก.ย.\": 9,\n",
    "    \"ต.ค.\": 10,\n",
    "    \"พ.ย.\": 11,\n",
    "    \"ธ.ค.\": 12,\n",
    "}\n",
    "\n",
    "def parse_thai_short_date(value: str) -> str:\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    match = re.match(r\"^(\\d{1,2})\\s+([ก-๙\\.]+)\\s+(\\d{2})$\", text)\n",
    "    if not match:\n",
    "        return \"\"\n",
    "\n",
    "    day = int(match.group(1))\n",
    "    month_text = match.group(2)\n",
    "    yy_be = int(match.group(3))  # e.g. 69 -> 2569 (B.E.)\n",
    "    month = THAI_MONTHS.get(month_text)\n",
    "    if month is None:\n",
    "        return \"\"\n",
    "\n",
    "    year_ad = (2500 + yy_be) - 543\n",
    "    try:\n",
    "        parsed = pd.Timestamp(year=year_ad, month=month, day=day)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "    return parsed.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "\n",
    "def clean_data_jobthai(job_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # province_name: remove \"จ.\" prefix\n",
    "    job_df[\"province_name\"] = (\n",
    "        job_df[\"province_name\"]\n",
    "        .fillna(\"\")\n",
    "        .astype(str)\n",
    "        .str.replace(r\"^\\s*จ\\.\\s*\", \"\", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # salary: extract min/max\n",
    "    salary_pattern = re.compile(\n",
    "        r\"^\\s*(\\d{1,3}(?:,\\d{3})*)\\s*-\\s*(\\d{1,3}(?:,\\d{3})*)(?:\\s*บาท)?\\s*$\"\n",
    "    )\n",
    "\n",
    "    salary_parts = job_df[\"salary\"].fillna(\"\").astype(str).str.extract(salary_pattern)\n",
    "    job_df[\"min_salary\"] = salary_parts[0].fillna(\"\").str.replace(\",\", \"\", regex=False)\n",
    "    job_df[\"max_salary\"] = salary_parts[1].fillna(\"\").str.replace(\",\", \"\", regex=False)\n",
    "\n",
    "    # 3) posted_date: convert Thai short date like to MM/DD/YYYY format\n",
    "    job_df[\"posted_date\"] = (\n",
    "        job_df[\"posted_date\"]\n",
    "        .fillna(\"\")\n",
    "        .astype(str)\n",
    "        .apply(parse_thai_short_date)\n",
    "    )\n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e07f91",
   "metadata": {},
   "source": [
    "## Clean Data JobThai Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c34d9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = clean_data_jobthai(jobs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75c8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df.to_csv(\"jobthai_jobs.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61969486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
