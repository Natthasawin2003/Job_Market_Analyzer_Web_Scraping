{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c788434d",
   "metadata": {},
   "source": [
    "# JobThai Web Scraper (Data Scientist)\n",
    "\n",
    "This notebook scrapes JobThai search results for **Data Scientist**, extracts structured fields, and saves them to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d932af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search] Starting page crawl: max_pages=10\n",
      "[Search] Page 1/10 -> request\n",
      "[Search] Page 1/10 -> found cards: 20\n",
      "[Search] Page 1/10 -> kept 7 | cumulative=7\n",
      "[Search] Page 2/10 -> request\n",
      "[Search] Page 2/10 -> found cards: 20\n",
      "[Search] Page 2/10 -> no keyword matches, stopping\n",
      "[Detail] Starting province lookup for 7 jobs\n",
      "[Detail] 1/7 (14.3%)\n",
      "[Detail] 2/7 (28.6%)\n",
      "[Detail] 3/7 (42.9%)\n",
      "[Detail] 4/7 (57.1%)\n",
      "[Detail] 5/7 (71.4%)\n",
      "[Detail] 6/7 (85.7%)\n",
      "[Detail] 7/7 (100.0%)\n",
      "[Done] Completed in 13.9s\n",
      "\n",
      "Total unique jobs: 7\n",
      "Saved CSV: G:\\Users\\Moss\\Documents\\PYTHON_PROJECT\\Job_Market_Analyzer_Web_Scraping\\Moss\\jobsdb_data-scientist_jobthai_all-locations_20260217.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>province_code</th>\n",
       "      <th>province_name</th>\n",
       "      <th>page</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>salary</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>job_url</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>01</td>\n",
       "      <td>กรุงเทพมหานคร</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Digital Dialogue Co., Ltd.</td>\n",
       "      <td>MRT พระราม 9, ศูนย์วัฒนธรรมแห่งประเทศไทย</td>\n",
       "      <td>ตามโครงสร้างบริษัทฯ</td>\n",
       "      <td>16 ก.พ. 69</td>\n",
       "      <td>https://www.jobthai.com/th/company/job/1685397</td>\n",
       "      <td>Data Scientist Digital Dialogue Co., Ltd. Digi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>01</td>\n",
       "      <td>กรุงเทพมหานคร</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>บริษัท จี.เอ็ม.เอส. คอร์เปอเรชั่น จำกัด</td>\n",
       "      <td>เขตยานนาวา กรุงเทพมหานคร</td>\n",
       "      <td>25,000 - 30,000 บาท</td>\n",
       "      <td>16 ก.พ. 69</td>\n",
       "      <td>https://www.jobthai.com/th/company/job/1822474</td>\n",
       "      <td>Data Scientist บริษัท จี.เอ็ม.เอส. คอร์เปอเรชั...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>28</td>\n",
       "      <td>จ.ปทุมธานี</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>บริษัท ดูโฮม จำกัด (มหาชน)</td>\n",
       "      <td>อ.เมืองปทุมธานี จ.ปทุมธานี</td>\n",
       "      <td>ตามโครงสร้างบริษัท/ประสบการณ์</td>\n",
       "      <td>16 ก.พ. 69</td>\n",
       "      <td>https://www.jobthai.com/th/company/job/1533922</td>\n",
       "      <td>Data Scientist บริษัท ดูโฮม จำกัด (มหาชน) บริษ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>01</td>\n",
       "      <td>กรุงเทพมหานคร</td>\n",
       "      <td>1</td>\n",
       "      <td>Specialist - DATA Scientist</td>\n",
       "      <td>บริษัท เคซีจี คอร์ปอเรชั่น จำกัด (มหาชน) / KCG...</td>\n",
       "      <td>BTS บางจาก, ปุณณวิถี</td>\n",
       "      <td>ตามโครงสร้างบริษัทฯ</td>\n",
       "      <td>16 ก.พ. 69</td>\n",
       "      <td>https://www.jobthai.com/th/company/job/1848401</td>\n",
       "      <td>Specialist - DATA Scientist บริษัท เคซีจี คอร์...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>01</td>\n",
       "      <td>กรุงเทพมหานคร</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist – AI &amp; Research</td>\n",
       "      <td>Lief Capital Asset Management Co., Ltd.</td>\n",
       "      <td>เขตคลองสาน กรุงเทพมหานคร</td>\n",
       "      <td>ตามโครงสร้างบริษัทฯ</td>\n",
       "      <td>16 ก.พ. 69</td>\n",
       "      <td>https://www.jobthai.com/th/company/job/1584351</td>\n",
       "      <td>Data Scientist – AI &amp; Research Lief Capital As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>01</td>\n",
       "      <td>กรุงเทพมหานคร</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist - WFH 95% + Office 5%</td>\n",
       "      <td>THiNKNET Co., Ltd.</td>\n",
       "      <td>BTS ช่องนนทรี, ศาลาแดง</td>\n",
       "      <td>25,000 - 70,000 บาท</td>\n",
       "      <td>16 ก.พ. 69</td>\n",
       "      <td>https://www.jobthai.com/th/company/job/1439476</td>\n",
       "      <td>Data Scientist - WFH 95% + Office 5% THiNKNET ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>01</td>\n",
       "      <td>กรุงเทพมหานคร</td>\n",
       "      <td>1</td>\n",
       "      <td>AI &amp; Data Science Specialist</td>\n",
       "      <td>บริษัท ทีคิวเอ็ม อินชัวร์รันส์ โบรคเกอร์ จำกัด...</td>\n",
       "      <td>เขตลาดพร้าว กรุงเทพมหานคร</td>\n",
       "      <td>ตามประสบการณ์</td>\n",
       "      <td>16 ก.พ. 69</td>\n",
       "      <td>https://www.jobthai.com/th/company/job/1680356</td>\n",
       "      <td>AI &amp; Data Science Specialist บริษัท ทีคิวเอ็ม ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          keyword province_code  province_name  page  \\\n",
       "0  Data Scientist            01  กรุงเทพมหานคร     1   \n",
       "1  Data Scientist            01  กรุงเทพมหานคร     1   \n",
       "2  Data Scientist            28     จ.ปทุมธานี     1   \n",
       "3  Data Scientist            01  กรุงเทพมหานคร     1   \n",
       "4  Data Scientist            01  กรุงเทพมหานคร     1   \n",
       "5  Data Scientist            01  กรุงเทพมหานคร     1   \n",
       "6  Data Scientist            01  กรุงเทพมหานคร     1   \n",
       "\n",
       "                              job_title  \\\n",
       "0                        Data Scientist   \n",
       "1                        Data Scientist   \n",
       "2                        Data Scientist   \n",
       "3           Specialist - DATA Scientist   \n",
       "4        Data Scientist – AI & Research   \n",
       "5  Data Scientist - WFH 95% + Office 5%   \n",
       "6          AI & Data Science Specialist   \n",
       "\n",
       "                                             company  \\\n",
       "0                         Digital Dialogue Co., Ltd.   \n",
       "1            บริษัท จี.เอ็ม.เอส. คอร์เปอเรชั่น จำกัด   \n",
       "2                         บริษัท ดูโฮม จำกัด (มหาชน)   \n",
       "3  บริษัท เคซีจี คอร์ปอเรชั่น จำกัด (มหาชน) / KCG...   \n",
       "4            Lief Capital Asset Management Co., Ltd.   \n",
       "5                                 THiNKNET Co., Ltd.   \n",
       "6  บริษัท ทีคิวเอ็ม อินชัวร์รันส์ โบรคเกอร์ จำกัด...   \n",
       "\n",
       "                                   location                         salary  \\\n",
       "0  MRT พระราม 9, ศูนย์วัฒนธรรมแห่งประเทศไทย            ตามโครงสร้างบริษัทฯ   \n",
       "1                  เขตยานนาวา กรุงเทพมหานคร            25,000 - 30,000 บาท   \n",
       "2                อ.เมืองปทุมธานี จ.ปทุมธานี  ตามโครงสร้างบริษัท/ประสบการณ์   \n",
       "3                      BTS บางจาก, ปุณณวิถี            ตามโครงสร้างบริษัทฯ   \n",
       "4                  เขตคลองสาน กรุงเทพมหานคร            ตามโครงสร้างบริษัทฯ   \n",
       "5                    BTS ช่องนนทรี, ศาลาแดง            25,000 - 70,000 บาท   \n",
       "6                 เขตลาดพร้าว กรุงเทพมหานคร                  ตามประสบการณ์   \n",
       "\n",
       "  posted_date                                         job_url  \\\n",
       "0  16 ก.พ. 69  https://www.jobthai.com/th/company/job/1685397   \n",
       "1  16 ก.พ. 69  https://www.jobthai.com/th/company/job/1822474   \n",
       "2  16 ก.พ. 69  https://www.jobthai.com/th/company/job/1533922   \n",
       "3  16 ก.พ. 69  https://www.jobthai.com/th/company/job/1848401   \n",
       "4  16 ก.พ. 69  https://www.jobthai.com/th/company/job/1584351   \n",
       "5  16 ก.พ. 69  https://www.jobthai.com/th/company/job/1439476   \n",
       "6  16 ก.พ. 69  https://www.jobthai.com/th/company/job/1680356   \n",
       "\n",
       "                                            raw_text  \n",
       "0  Data Scientist Digital Dialogue Co., Ltd. Digi...  \n",
       "1  Data Scientist บริษัท จี.เอ็ม.เอส. คอร์เปอเรชั...  \n",
       "2  Data Scientist บริษัท ดูโฮม จำกัด (มหาชน) บริษ...  \n",
       "3  Specialist - DATA Scientist บริษัท เคซีจี คอร์...  \n",
       "4  Data Scientist – AI & Research Lief Capital As...  \n",
       "5  Data Scientist - WFH 95% + Office 5% THiNKNET ...  \n",
       "6  AI & Data Science Specialist บริษัท ทีคิวเอ็ม ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import parse_qs, urlencode, urlparse, urlunparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "SEARCH_URL = \"https://www.jobthai.com/th/jobs?keyword=Data%20Scientist&page=1&orderBy=RELEVANCE_SEARCH\"\n",
    "OUTPUT_CSV = \"jobsdb_data-scientist_jobthai_all-locations_20260217.csv\"\n",
    "MAX_PAGES = 10\n",
    "SLEEP_SECONDS = 1.0\n",
    "DETAIL_SLEEP_SECONDS = 0.5\n",
    "\n",
    "\n",
    "def normalize_province_code(value) -> str:\n",
    "    text = str(value).strip()\n",
    "    if text.isdigit():\n",
    "        number = int(text)\n",
    "        if number <= 0:\n",
    "            raise ValueError(f\"Province must be positive, got: {value}\")\n",
    "        return f\"{number:02d}\"\n",
    "    raise ValueError(f\"Invalid province code: {value}\")\n",
    "\n",
    "\n",
    "def normalize_for_match(text: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \" \", text.lower()).strip()\n",
    "\n",
    "\n",
    "def keyword_match_groups_from_query(keyword: str) -> list[list[str]]:\n",
    "    term_variants = {\n",
    "        \"data\": [\"data\"],\n",
    "        \"scientist\": [\"scientist\", \"science\", \"scien\", \"scient\"],\n",
    "        \"science\": [\"science\", \"scientist\", \"scien\", \"scient\"],\n",
    "        \"engineer\": [\"engineer\", \"engineering\", \"eng\"],\n",
    "        \"analyst\": [\"analyst\", \"analytics\", \"analysis\"],\n",
    "        \"developer\": [\"developer\", \"development\", \"dev\"],\n",
    "    }\n",
    "\n",
    "    tokens = [token for token in normalize_for_match(keyword).split() if token]\n",
    "    groups = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in term_variants:\n",
    "            groups.append(term_variants[token])\n",
    "        else:\n",
    "            groups.append([token])\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def title_matches_keyword(title: str, keyword_groups: list[list[str]]) -> bool:\n",
    "    if not keyword_groups:\n",
    "        return True\n",
    "\n",
    "    title_norm = normalize_for_match(title)\n",
    "    return all(any(variant in title_norm for variant in group) for group in keyword_groups)\n",
    "\n",
    "\n",
    "def update_query_in_url(url: str, **params) -> str:\n",
    "    parsed = urlparse(url)\n",
    "    query = parse_qs(parsed.query)\n",
    "\n",
    "    for key, value in params.items():\n",
    "        query[key] = [str(value)]\n",
    "\n",
    "    new_query = urlencode(query, doseq=True)\n",
    "    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, new_query, parsed.fragment))\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def extract_salary(text: str) -> str:\n",
    "    patterns = [\n",
    "        r\"\\d[\\d,\\s]*\\s*-\\s*\\d[\\d,\\s]*\\s*บาท\",\n",
    "        r\"\\d[\\d,\\s]*\\s*บาท\",\n",
    "        r\"ตามโครงสร้างบริษัทฯ\",\n",
    "        r\"ตามประสบการณ์\",\n",
    "        r\"ตามตกลง\",\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return clean_text(match.group(0))\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_posted_date(text: str) -> str:\n",
    "    match = re.search(r\"\\b\\d{1,2}\\s+[ก-๙A-Za-z\\.]+\\s+\\d{2}\\b\", text)\n",
    "    return clean_text(match.group(0)) if match else \"\"\n",
    "\n",
    "\n",
    "def pick_text(parent, selectors: list[str]) -> str:\n",
    "    for selector in selectors:\n",
    "        element = parent.select_one(selector)\n",
    "        if element:\n",
    "            text = clean_text(element.get_text(\" \", strip=True))\n",
    "            if text:\n",
    "                return text\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def guess_location(lines: list[str], title: str, company: str, salary: str) -> str:\n",
    "    priority_keywords = [\"เขต\", \"กรุงเทพ\", \"จังหวัด\", \"อำเภอ\", \"อ.\", \"ต.\"]\n",
    "    transit_keywords = [\"BTS\", \"MRT\", \"SRT\", \"BRT\", \"Airport Rail Link\"]\n",
    "\n",
    "    for line in lines:\n",
    "        if line in {title, company, salary}:\n",
    "            continue\n",
    "        if any(keyword in line for keyword in priority_keywords):\n",
    "            return line\n",
    "\n",
    "    for line in lines:\n",
    "        if line in {title, company, salary}:\n",
    "            continue\n",
    "        if any(keyword in line for keyword in transit_keywords):\n",
    "            return line\n",
    "\n",
    "    if salary and salary in lines:\n",
    "        salary_idx = lines.index(salary)\n",
    "        for idx in range(salary_idx - 1, -1, -1):\n",
    "            candidate = lines[idx]\n",
    "            if candidate not in {title, company}:\n",
    "                return candidate\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_card_from_title(title_node, page_num: int, keyword: str) -> dict:\n",
    "    title = clean_text(title_node.get_text(\" \", strip=True))\n",
    "\n",
    "    anchor = title_node.find_parent(\"a\", href=True)\n",
    "    href = anchor.get(\"href\", \"\") if anchor else \"\"\n",
    "    job_url = href if href.startswith(\"http\") else f\"https://www.jobthai.com{href}\"\n",
    "\n",
    "    card = anchor if anchor is not None else title_node\n",
    "\n",
    "    company = pick_text(card, [\n",
    "        'span[id^=\"job-list-company-name-\"]',\n",
    "        'h2.ohgq7e-0.enAWkF',\n",
    "    ])\n",
    "\n",
    "    location = pick_text(card, [\n",
    "        \"h3#location-text\",\n",
    "        \"h3.location-text\",\n",
    "    ])\n",
    "\n",
    "    salary = pick_text(card, [\n",
    "        \"span.salary-text\",\n",
    "        \"div.msklqa-20\",\n",
    "        \"div.msklqa-17\",\n",
    "    ])\n",
    "\n",
    "    posted_date = pick_text(card, [\n",
    "        \"span.msklqa-9\",\n",
    "    ])\n",
    "\n",
    "    raw_lines = [clean_text(x) for x in card.get_text(\"\\n\", strip=True).splitlines() if clean_text(x)]\n",
    "    raw_text = clean_text(\" \".join(raw_lines))\n",
    "\n",
    "    if not salary:\n",
    "        salary = extract_salary(raw_text)\n",
    "    if not posted_date:\n",
    "        posted_date = extract_posted_date(raw_text)\n",
    "    if not location:\n",
    "        location = guess_location(raw_lines, title=title, company=company, salary=salary)\n",
    "\n",
    "    return {\n",
    "        \"keyword\": keyword,\n",
    "        \"page\": page_num,\n",
    "        \"job_title\": title,\n",
    "        \"company\": company,\n",
    "        \"location\": location,\n",
    "        \"salary\": salary,\n",
    "        \"posted_date\": posted_date,\n",
    "        \"job_url\": job_url,\n",
    "        \"raw_text\": raw_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_province_from_detail(job_url: str, headers: dict) -> tuple[str, str]:\n",
    "    try:\n",
    "        response = requests.get(job_url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "    except Exception:\n",
    "        return \"\", \"\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    province_links = soup.select('a[id^=\"company-1-tag-\"][href*=\"province=\"]')\n",
    "\n",
    "    for link in province_links:\n",
    "        href = link.get(\"href\", \"\")\n",
    "        name = clean_text(link.get_text(\" \", strip=True))\n",
    "\n",
    "        if not href or not name:\n",
    "            continue\n",
    "\n",
    "        province_value = parse_qs(urlparse(href).query).get(\"province\", [\"\"])[0]\n",
    "        if not province_value or not province_value.isdigit():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            code = normalize_province_code(province_value)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        return code, name\n",
    "\n",
    "    return \"\", \"\"\n",
    "\n",
    "\n",
    "def scrape_jobthai_all_locations(\n",
    "    search_url: str,\n",
    "    max_pages: int = 10,\n",
    "    sleep_seconds: float = 1.0,\n",
    "    detail_sleep_seconds: float = 0.5,\n",
    ") -> pd.DataFrame:\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"th-TH,th;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    }\n",
    "\n",
    "    keyword = parse_qs(urlparse(search_url).query).get(\"keyword\", [\"\"])[0]\n",
    "    keyword_groups = keyword_match_groups_from_query(keyword)\n",
    "\n",
    "    all_rows = []\n",
    "    seen_urls = set()\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"[Search] Starting page crawl: max_pages={max_pages}\")\n",
    "\n",
    "    for page_num in range(1, max_pages + 1):\n",
    "        page_url = update_query_in_url(search_url, page=page_num)\n",
    "        print(f\"[Search] Page {page_num}/{max_pages} -> request\")\n",
    "\n",
    "        response = requests.get(page_url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if \"nodata=true\" in response.url.lower():\n",
    "            print(f\"[Search] Page {page_num}/{max_pages} -> nodata=true, stopping\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        title_nodes = soup.select('h2[id^=\"job-card-item-\"]')\n",
    "        print(f\"[Search] Page {page_num}/{max_pages} -> found cards: {len(title_nodes)}\")\n",
    "\n",
    "        page_rows = []\n",
    "        for title_node in title_nodes:\n",
    "            row = parse_card_from_title(\n",
    "                title_node,\n",
    "                page_num=page_num,\n",
    "                keyword=keyword,\n",
    "            )\n",
    "            if not row[\"job_url\"]:\n",
    "                continue\n",
    "            if not title_matches_keyword(row[\"job_title\"], keyword_groups):\n",
    "                continue\n",
    "            if row[\"job_url\"] in seen_urls:\n",
    "                continue\n",
    "\n",
    "            seen_urls.add(row[\"job_url\"])\n",
    "            page_rows.append(row)\n",
    "\n",
    "        if not page_rows:\n",
    "            print(f\"[Search] Page {page_num}/{max_pages} -> no keyword matches, stopping\")\n",
    "            break\n",
    "\n",
    "        all_rows.extend(page_rows)\n",
    "        print(\n",
    "            f\"[Search] Page {page_num}/{max_pages} -> kept {len(page_rows)} | cumulative={len(all_rows)}\"\n",
    "        )\n",
    "\n",
    "        if sleep_seconds > 0:\n",
    "            time.sleep(sleep_seconds)\n",
    "\n",
    "    total_details = len(all_rows)\n",
    "    print(f\"[Detail] Starting province lookup for {total_details} jobs\")\n",
    "\n",
    "    for index, row in enumerate(all_rows, start=1):\n",
    "        province_code, province_name = extract_province_from_detail(row[\"job_url\"], headers=headers)\n",
    "        row[\"province_code\"] = province_code\n",
    "        row[\"province_name\"] = province_name\n",
    "\n",
    "        if total_details <= 50 or index % 10 == 0 or index == total_details:\n",
    "            percent = (index / total_details) * 100 if total_details else 100\n",
    "            print(f\"[Detail] {index}/{total_details} ({percent:.1f}%)\")\n",
    "\n",
    "        if detail_sleep_seconds > 0:\n",
    "            time.sleep(detail_sleep_seconds)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[Done] Completed in {elapsed:.1f}s\")\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "\n",
    "    if not df.empty:\n",
    "        df = df[\n",
    "            [\n",
    "                \"keyword\",\n",
    "                \"province_code\",\n",
    "                \"province_name\",\n",
    "                \"page\",\n",
    "                \"job_title\",\n",
    "                \"company\",\n",
    "                \"location\",\n",
    "                \"salary\",\n",
    "                \"posted_date\",\n",
    "                \"job_url\",\n",
    "                \"raw_text\",\n",
    "            ]\n",
    "        ].drop_duplicates(subset=[\"job_url\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "jobs_df = scrape_jobthai_all_locations(\n",
    "    SEARCH_URL,\n",
    "    max_pages=MAX_PAGES,\n",
    "    sleep_seconds=SLEEP_SECONDS,\n",
    "    detail_sleep_seconds=DETAIL_SLEEP_SECONDS,\n",
    ")\n",
    "print(f\"\\nTotal unique jobs: {len(jobs_df)}\")\n",
    "\n",
    "output_path = Path(OUTPUT_CSV)\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "jobs_df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Saved CSV: {output_path.resolve()}\")\n",
    "\n",
    "jobs_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
